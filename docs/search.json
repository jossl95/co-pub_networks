[
  {
    "objectID": "src/docs/content/process_gender.html",
    "href": "src/docs/content/process_gender.html",
    "title": "Data Processing Gender",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  605350 32.4    1371670 73.3         NA   715802 38.3\nVcells 1118211  8.6    8388608 64.0      16384  2012236 15.4\n\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(httr2)\nlibrary(rvest)\nlibrary(xml2)\nlibrary(purrr)\nlibrary(genderizeR)\nlibrary(dotenv)\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()"
  },
  {
    "objectID": "src/docs/content/process_gender.html#getting-started",
    "href": "src/docs/content/process_gender.html#getting-started",
    "title": "Data Processing Gender",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  605350 32.4    1371670 73.3         NA   715802 38.3\nVcells 1118211  8.6    8388608 64.0      16384  2012236 15.4\n\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(httr2)\nlibrary(rvest)\nlibrary(xml2)\nlibrary(purrr)\nlibrary(genderizeR)\nlibrary(dotenv)\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()"
  },
  {
    "objectID": "src/docs/content/process_gender.html#functions",
    "href": "src/docs/content/process_gender.html#functions",
    "title": "Data Processing Gender",
    "section": "Functions",
    "text": "Functions\n\nCall Meertens Voornamen Databank\n\nis_ok = function(resp) resp_status(resp) &gt;= 200 && resp_status(resp) &lt; 300\n\nrequest_gender = function(\n    first_name,\n    base = \"https://nvb.meertens.knaw.nl/naam/is/\",\n    pause = 0.5\n  ){\n  # configure url for scraping\n  url  = paste0(base, URLencode(tolower(first_name), reserved = TRUE))\n\n  # configure user agent\n  ua = paste(\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 15_5)\",\n    \"AppleWebKit/537.36 (KHTML, like Gecko)\",\n    \"Chrome/129.0.0.0 Safari/537.36\"\n  )\n\n  req = request(url) |&gt;\n    req_user_agent(ua) |&gt;\n    req_timeout(30) |&gt;\n    req_headers(\n      \"Accept\" = paste0(\"text/html,application/xhtml+xml,\",\n                        \"application/xml;q=0.9,*/*;q=0.8\"),\n      \"Accept-Language\" = \"nl,en;q=0.8\"\n    ) |&gt;\n    # Retry on 429/5xx, and *also* on network hiccups:\n    req_retry(\n      max_tries = 4,\n      backoff = ~ runif(1, 0.5, 1.2) * (2 ^ (.x - 1)),  # jittered exponential\n      is_transient = function(resp) {\n        code &lt;- resp_status(resp)\n        isTRUE(code == 429L || (code &gt;= 500L & code &lt; 600L))\n      }\n    )\n\n  # polite pause + jitter\n  if (pause &gt; 0) Sys.sleep(pause + runif(1, 0, 0.4))\n\n  # CRITICAL: don't throw on transport errors\n  resp &lt;- tryCatch(\n    req_perform(req),\n    error = function(e) {\n      attr(e, \"nvb_url\") &lt;- url\n      e\n    }\n  )\n\n  # Return a uniform list the caller can inspect\n  if (inherits(resp, \"error\")) {\n    res = list(\n      ok = FALSE,\n      status = NA_integer_,\n      url = url,\n      resp = NULL,\n      error = resp\n    )\n  } else {\n    res = list(\n      ok = is_ok(resp),\n      status = resp_status(resp),\n      url = url,\n      resp = resp,\n      error = NULL\n    )\n  }\n  return(res)\n}\n\n\n\nExtract Gender Information\n\nextract_gender_information = function(\n    resp,\n    first_name\n  ){\n  # extract all the tables on the page\n  html = read_html(resp_body_string(resp))\n  tables = html_table(html, header=TRUE)\n\n  # select the first table if a table if found\n  if (length(tables) == 0) stop(\"No tables were found\")\n  tab = tables[[1]]\n\n  # extract information from table\n  male_count = tab[1, 3] |&gt; pull()\n  male_count = ifelse(male_count == '--', 0, as.numeric(male_count))\n  female_count = tab[5, 3] |&gt; pull() \n  female_count = ifelse(female_count == '--', 0, as.numeric(female_count))\n  probability_male = male_count / (female_count + male_count) \n\n  # configure results table\n  res = tibble::tribble(\n    ~first_name, ~male_count, ~female_count, ~probability_male,\n    first_name, male_count,  female_count,  probability_male\n  )\n  return(res)\n}\n\n\n\nHelper functions\n\nsafe_extract = purrr::possibly(\n  extract_gender_information,\n  otherwise = tibble::tibble(\n    first_name       = NA_character_,\n    male_count       = NA_integer_,\n    female_count     = NA_integer_,\n    probability_male = NA_real_\n  )\n)\n\nget_gender_row = function(name, gender) {\n  # If cached, return from cache\n  if (name %in% gender$first_name) {\n    return(gender |&gt; filter(first_name == name))\n  }\n\n  r &lt;- request_gender(name)\n\n  # If transport error or HTTP not OK, surface status & keep going\n  if (!isTRUE(r$ok)) {\n    return(tibble(\n      first_name       = NA_character_,\n      male_count       = NA_integer_,\n      female_count     = NA_integer_,\n      probability_male = NA_real_\n    ))\n  }\n\n  out &lt;- safe_extract(r$resp, name) |&gt;\n    mutate(first_name = name)\n\n  out\n}\n\n\n\nPatch Difficult Names\n\npatch_gender_on_splits = function(gender){\n  # set gender cache\n  gender_cache = gender |&gt; drop_na()\n  # select authors without gender, and split their names\n  selection = gender |&gt;\n    filter(is.na(male_count)) |&gt;\n    mutate(first_name_split = str_split(first_name, ' ')) |&gt;\n    unnest_longer(first_name_split) |&gt;\n    select(first_name, first_name_split)\n\n  # get first names from selection\n  first_names = selection |&gt;\n    select(first_name_split) |&gt;\n    pull() |&gt;\n    unique()\n\n  # patch gender --------------------------------------\n  gender_patch = purrr::map_dfr(\n    first_names, get_gender_row, gender = gender_cache\n  )\n\n  # aggregate gender information\n  gender_patch = selection |&gt;\n    left_join(\n      gender_patch, \n      by=join_by(first_name_split == first_name)\n    ) |&gt;\n    drop_na() |&gt;\n    # take the average gender count and probablity\n    # for names where both splits yielded a gender\n    # result\n    group_by(first_name) |&gt;\n    summarise(\n      male_count = as.integer(mean(male_count)),\n      female_count = as.integer(mean(female_count)),\n      probability_male = mean(probability_male)\n    ) |&gt;\n    ungroup() \n\n  gender |&gt; rows_update(gender_patch)\n}\n\n\n\nClean Gender Information\n\nclean_gender = function(data){\n  data |&gt;\n    mutate(\n      gender = ifelse(\n        probability_male &gt;= 0.5,\n        'male','female'\n      ),\n      count = ifelse(\n        gender == 'male',\n        male_count, female_count\n      ),\n      prob = ifelse(\n        gender == 'male',\n        probability_male, 1 - probability_male\n      )\n    ) |&gt;\n    select(first_name, gender, prob, count)\n}\n\n\n\nPatch Missing Gender\n\npatch_missing_gender = function(data){\n  female_names = c(\n    \"Alaxandra\",   \"Alinson\",     \"Avyanthi\",\n    \"Brunilda\",    \"Busisiwe\",    \"Diliara\",      \n    \"Dolive\",      \"Echo\",        \"Guangyu\",\n    # mistake in name, has been patched with _create_name_corrections\n    \"Guangye\", \n    \"Gul-i-Hina\",  \"Haebin\",      \"Haisu\",\n    # Phoebe Kisibi Mbasalaki was incorrectly coded, \n    # has been patched with _create_name_corrections \n    \"Kisubi\",      \"Pheobe\",\n    \"Liubov\",      \"Madalina\",    \"Majolijn\",    \n    \"Mansoureh\",   \"Nankyung\",    \"Nilmawati\",   \n    \"Nodira\",      \"Noyonika\",    \"Radostina\",   \n    \"Rojika\",      \"Rozenmarijn\", \"Sayoni\",\n    \"Seonoki\",     \"Shelliann\",   \"Shiming\",     \n    \"Siggie\",      \"Siztine\",     \"Sungmi\",      \n    \"Talinta\",     \"Teana\",       \"Xingna\",\n    \"Yuliia\",      \"Zhiyi\"\n  )\n\n  male_names = c(\n    \"Alborno\",     \"Chenchen\",    \"Chendi\",\n    \"Chunglin\",    \"Chuyu\",       \"Diliara\",\n    \"Gjovalin\",    \"Kirils\",      \"Kyohee\",\n    \"Madhud\",      \"Quichen\",     \"Soeren\",\n    \"Teana\",       \"Tanzhe\",     \"Vishwesh\",\n    \"Weverthon\"\n  )\n\n  data |&gt;\n    mutate(\n      gender = case_when(\n        !is.na(gender) ~ gender,\n        first_name %in% female_names ~ 'female',\n        first_name %in% male_names ~ 'male',\n        .default = gender\n      )\n    )\n}\n\n\ngenderize_names = function(idx) {\n  # load cached gender information\n  gender_cache = readRDS(file.path('data', 'utils', \"genderizer_cache.Rds\"))\n\n  # load api key from secrets file\n  dotenv::load_dot_env()\n  APIKEY &lt;- Sys.getenv(\"GENDERIZE_API_KEY\")\n\n  # select uncached names\n  idx =  idx |&gt; filter(!term %in% gender_cache$name)\n\n  # select first_names\n  first_names = idx$term |&gt; na.omit() |&gt; unique()\n  first_names = first_names\n\n  # fetch gender results\n  hold = c()\n  for (name in first_names){\n    resp = genderizeAPI(name, apikey = APIKEY)\n    hold[[name]] = resp$response\n  }\n\n  # combine cache with results and put new cache results\n  res = bind_rows(gender_cache, bind_rows(hold)) |&gt;\n    distinct(.keep_all = TRUE)\n  saveRDS(res, file.path('data', 'utils', \"genderizer_cache.Rds\"))\n\n  return(res)\n}\n\n\nscrape_gender = function(idx) {\n  # load gender cache\n  gender_cache = readRDS(file.path(\"data\", \"utils\", \"nvb_gender.Rds\")) |&gt; \n    drop_na()\n\n  first_names = idx |&gt; pull(first_name) |&gt; unique() |&gt; na.omit()\n\n  # scrape gender results\n  res = purrr::map_dfr(\n      first_names, \n      get_gender_row, \n      gender = gender_cache\n    ) |&gt;\n    patch_gender_on_splits() \n\n  # put gender scrape results\n  saveRDS(res |&gt; drop_na(), file.path('data', 'utils', \"nvb_gender.Rds\"))\n\n  # clean gender results\n  res = res |&gt; \n    clean_gender() |&gt;\n    patch_missing_gender()\n\n  return(res)\n}\n\n\nharmonize_gender = function(gender) {\n  gender |&gt;\n    select(\n      first_name, term, starts_with('gender'), \n      starts_with('count'), starts_with('prob')\n    ) |&gt;\n    distinct(first_name, term, .keep_all=TRUE) |&gt;\n    mutate(\n      has_multiple = str_detect(first_name, '( |-)'),\n      has_mismatch = gender.x != gender.y\n    ) |&gt;\n    drop_na(first_name) |&gt;\n    filter(!(has_multiple & has_mismatch & str_detect(term, '.'))) |&gt;\n    mutate(\n      gender = case_when(\n        is.na(gender.y) ~ gender.x,\n        .default = gender.y\n      ),\n      prob = case_when(\n        is.na(probability) ~ prob,\n        .default = probability\n      ),\n      count = case_when(\n        is.na(count.y) ~ count.x,\n        .default = count.y\n      )\n    ) |&gt;\n    group_by(first_name) |&gt;\n    summarise(\n      gender = first(gender),\n      prob = mean(prob), \n      count = sum(count)\n    )\n}"
  },
  {
    "objectID": "src/docs/content/process_gender.html#application",
    "href": "src/docs/content/process_gender.html#application",
    "title": "Data Processing Gender",
    "section": "Application",
    "text": "Application\n\ndir = file.path('data', 'processed')\nfile = list.files(dir, pattern = 'names.Rds')\nnames = readRDS(file.path(dir, file[length(file)]))\n\nidx = names |&gt;\n  mutate(term = first_name |&gt; \n    str_to_lower() |&gt; \n    str_split('( |-)')\n  ) |&gt;\n  unnest_longer(term)\nres = scrape_gender(idx)\n\nMatching, by = \"first_name\"\n\nres2 = genderize_names(idx)\n\ngender = idx |&gt;\n  left_join(res) |&gt;\n  left_join(res2, by=join_by(term == name), suffix = c(\".x\", \".y\")) |&gt;\n  harmonize_gender()\n\nJoining with `by = join_by(first_name)`\n\nfsaveRDS(gender, \"gender\")\n\n[1] \"SAVING: ./data/processed/20251027gender.Rds\""
  },
  {
    "objectID": "src/docs/content/combine_data.html",
    "href": "src/docs/content/combine_data.html",
    "title": "Data Preparation OpenAlex-ID",
    "section": "",
    "text": "Tip\n\n\n\nrough first working code for the collection of scholar_ids using openalexr\nI optimized the algorithm to not just select the top case, but allows for multiple ids (in rows) per person, in the decision rules i use: - semantic similarity - matches of university_id and institution_id"
  },
  {
    "objectID": "src/docs/content/combine_data.html#getting-started",
    "href": "src/docs/content/combine_data.html#getting-started",
    "title": "Data Preparation OpenAlex-ID",
    "section": "Getting Started",
    "text": "Getting Started\n\n# clear the global environment\nrm(list = ls())\ngc()\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(openalexR)\nlibrary(rvest)\nlibrary(jsonlite)\nlibrary(cli)\n\n# load custom functions \nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()\n\noptions(openalexR.mailto = \"jos.slabbekoorn@ru.nl\")\n\n\ndir = file.path('data', 'processed')\nfiles = list.files(dir)\ndata = list(\n    scholars = readRDS(file.path(dir, files[str_detect(files, 'scholarid')])),\n    ethnicity = readRDS(file.path(dir, files[str_detect(files, 'ethnicity')])),\n    gender = readRDS(file.path(dir, files[str_detect(files, 'gender')])),\n    names = readRDS(file.path(dir, files[str_detect(files, 'names')])),\n    id = readRDS(file.path('data', 'raw_data', '20251015oascholars.Rds')),\n    works = readRDS(file.path('data', 'raw_data', '20251016oaworks.Rds'))\n)\n\n\n# scholars = fread(file.)\n\n\ndem = data[['scholars']] |&gt;\n    mutate(\n        year = as.integer(year(date) - 2000)\n    ) |&gt;\n    arrange(naam, date) |&gt;\n    select(-university, -date, -google_scholar_id) |&gt;\n    pivot_wider(\n        names_from  = year,\n        values_from = c(email_adres, universiteit, functie, discipline),\n        names_glue  = \"{.value}.{year}\",\n        values_fill = NA,\n        values_fn = list(\n        email_adres   = ~ if (all(is.na(.x))) NA_character_ else str_c(unique(na.omit(.x)), collapse = \"; \"),\n        universiteit  = ~ if (all(is.na(.x))) NA_character_ else first(na.omit(.x)),\n        functie       = ~ if (all(is.na(.x))) NA_character_ else first(na.omit(.x)),\n        discipline    = ~ if (all(is.na(.x))) NA_character_ else first(na.omit(.x))\n        )\n    ) |&gt;\n    select(\n        naam, ends_with('22'), ends_with('24'), ends_with('25')\n    )\n\n\n# add ids\ndems = data$id |&gt;\n    bind_rows() |&gt;\n    arrange(query_name, works_count) |&gt;\n    select(query_name, id) |&gt;\n    # select the row with the most works\n    distinct(query_name, .keep_all = TRUE) |&gt;\n    # merge in demographics\n    rename(naam = query_name) |&gt;\n    right_join(dem) |&gt;\n    # merge in names\n    left_join(data$names) |&gt;\n    relocate(initials:maiden_name, .after=id) |&gt;\n    # merge in gender\n    left_join(data$gender |&gt; select(-count, -prob)) |&gt;\n    relocate(gender, .after=maiden_name) |&gt;\n    # merge in ethnicity\n    left_join(data$ethnicity |&gt; select(-name_count)) |&gt;\n    relocate(origin:dutch, .after=gender) |&gt;\n    distinct(naam, id, .keep_all=TRUE) |&gt;\n    mutate(has_oa_id = !is.na(id)) |&gt;\n    drop_na(id) |&gt;\n    distinct(id, .keep_all=TRUE)\n\n\nids = na.omit(unique(dems$id))\nworks = list()\nfor (id in ids){\n    works[[id]] = data$works[[id]]\n}\n\n\nauthor_list = list()\ndf_scholars = bind_rows(data$id)\nfor (id_ in ids){\n    tab = df_scholars |&gt; filter(id == id_)\n    author_list[[id_]] = tab\n}\n\n\nscholars = list(\n    demographics=dems, \n    scholars_oa = author_list,\n    works = works\n)\n\n\nsave(scholars, file = file.path('data', 'processed', '20251017scholars.Rds'))"
  },
  {
    "objectID": "src/docs/content/fix_institution.html",
    "href": "src/docs/content/fix_institution.html",
    "title": "Create patch for institutions",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(openalexR)\nlibrary(rvest)\nlibrary(jsonlite)\nlibrary(cli)\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()\n\noptions(openalexR.mailto = \"jos.slabbekoorn@ru.nl\")\n\n\ndir = file.path('data', 'processed')\nfile = list.files(dir, pattern = 'names.Rds')\nnames = freadRDS(file.path(dir, file[length(file)]))\n\n\nfile = list.files(dir, pattern = 'scholarid.Rds')\nscholar = freadRDS(file.path(dir, file)) |&gt;\n  select(naam, university) |&gt;\n  distinct(.keep_all = TRUE) |&gt;\n  unnest_longer(university) \n\n\ndir = file.path('data', 'raw_data')\nfile = list.files(dir, pattern='oaworks')\npubs = bind_rows(freadRDS(file.path(dir, file[length(file)])))\n\nfile = list.files(dir, pattern='oascholars')\ndems = bind_rows(freadRDS(file.path(dir, file[length(file)]))) |&gt;\n  select(id, last_known_institutions)\n\n\nunnest_authors &lt;- function(pubs) {\n  pubs |&gt;\n    # 1) Make sure each element of `authorships` is a tibble (handles NULLs/lists)\n    mutate(\n      authorships = map(authorships, ~\n        if (is.null(.x)) tibble()\n        else if (is.data.frame(.x)) as_tibble(.x)\n        else tibble::as_tibble(.x)\n      )\n    ) |&gt;\n    # 2) One row per author record (keeps pubs with 0 authors via keep_empty)\n    unnest(authorships, keep_empty = TRUE, names_sep = \"_\") |&gt;\n    # 3) Make sure each element of `affiliations` is a tibble (handles NULLs/lists)\n    mutate(\n      authorships_affiliations = map(authorships_affiliations, ~\n        if (is.null(.x)) tibble()\n        else if (is.data.frame(.x)) as_tibble(.x)\n        else tibble::as_tibble(.x)\n      )\n    ) |&gt;\n    # 4) One row per author record (keeps pubs with 0 authors via keep_empty)\n    unnest(authorships_affiliations, keep_empty = TRUE, names_sep = \"_\")  |&gt;\n    # 3) Make sure each element of `affiliations` is a tibble (handles NULLs/lists)\n    mutate(\n      last_known_institutions = map(last_known_institutions, ~\n        if (is.null(.x)) tibble()\n        else if (is.data.frame(.x)) as_tibble(.x)\n        else tibble::as_tibble(.x)\n      )\n    ) |&gt;\n    # 4) One row per author record (keeps pubs with 0 authors via keep_empty)\n    unnest(last_known_institutions, keep_empty = TRUE, names_sep = \"_\")\n}\n\n# pubs_authors &lt;- unnest_authors(pubs, dems) |&gt;\n#   filter(author_id == authorships_id) |&gt;\n#   filter(authorhships_affiliations)\n\n\npubs_authors = pubs |&gt;\n  left_join(dems, by = join_by(author_id == id)) |&gt;\n  unnest_authors()|&gt;\n  filter(author_id == authorships_id) |&gt;\n  filter(\n    is.na(authorships_affiliations_id) |\n    authorships_affiliations_id == last_known_institutions_id\n  )\n\n\nuniversities = c(\n    \"https://openalex.org/I865915315\", \"https://openalex.org/I887064364\",\n    \"https://openalex.org/I121797337\", \"https://openalex.org/I145872427\", \n    \"https://openalex.org/I193662353\", \"https://openalex.org/I913958620\", \n    \"https://openalex.org/I193700539\", \"https://openalex.org/I169381384\", \n    \"https://openalex.org/I913481162\"\n)\n\ndates = pubs_authors |&gt;\n    distinct(\n      authorships_display_name, authorships_id, authorships_affiliations_id, \n      authorships_affiliations_display_name, publication_date,\n      .keep_all=TRUE\n    ) |&gt;\n    filter(authorships_affiliations_id %in% universities) |&gt;\n    arrange(publication_date) |&gt;\n    group_by(authorships_id, authorships_affiliations_display_name, authorships_affiliations_id) |&gt;\n    summarize(\n        authorships_display_name = first(authorships_display_name),\n        first_date = first(publication_date) ,\n        last_date = last(publication_date)\n    ) |&gt;\n    mutate(\n      first_date = first_date %m-% months(18),\n      last_date = last_date %m+% months(6)\n    )\n\n\ntimepoints &lt;- c(\"2019-09-01\", \"2022-12-19\", \"2024-04-19\", \"2025-10-02\")\ntp &lt;- ymd(timepoints)\n\nd = dates |&gt;\n  tidyr::expand_grid(timepoint = tp) |&gt;\n  arrange(timepoint) |&gt;\n  mutate(\n    in_range = between(timepoint, first_date, last_date),\n    year = as.integer(year(timepoint) - 2000) \n  ) |&gt;\n  filter(in_range) |&gt;\n  distinct(authorships_display_name, authorships_id, authorships_display_name, authorships_affiliations_display_name, year) |&gt;\n  pivot_wider(\n    id_cols = c(authorships_display_name, authorships_id),\n    names_from = year,\n    names_prefix = 'affiliation_',\n    values_from = authorships_affiliations_display_name,\n    values_fn = list(authorships_affiliations_display_name = \\(x) list(sort(unique(na.omit(x)))))\n  ) |&gt;\n  arrange(authorships_display_name)\n\nfsaveRDS(d, 'institutions_patch')"
  },
  {
    "objectID": "src/docs/content/fix_institution.html#getting-started",
    "href": "src/docs/content/fix_institution.html#getting-started",
    "title": "Create patch for institutions",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(openalexR)\nlibrary(rvest)\nlibrary(jsonlite)\nlibrary(cli)\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()\n\noptions(openalexR.mailto = \"jos.slabbekoorn@ru.nl\")\n\n\ndir = file.path('data', 'processed')\nfile = list.files(dir, pattern = 'names.Rds')\nnames = freadRDS(file.path(dir, file[length(file)]))\n\n\nfile = list.files(dir, pattern = 'scholarid.Rds')\nscholar = freadRDS(file.path(dir, file)) |&gt;\n  select(naam, university) |&gt;\n  distinct(.keep_all = TRUE) |&gt;\n  unnest_longer(university) \n\n\ndir = file.path('data', 'raw_data')\nfile = list.files(dir, pattern='oaworks')\npubs = bind_rows(freadRDS(file.path(dir, file[length(file)])))\n\nfile = list.files(dir, pattern='oascholars')\ndems = bind_rows(freadRDS(file.path(dir, file[length(file)]))) |&gt;\n  select(id, last_known_institutions)\n\n\nunnest_authors &lt;- function(pubs) {\n  pubs |&gt;\n    # 1) Make sure each element of `authorships` is a tibble (handles NULLs/lists)\n    mutate(\n      authorships = map(authorships, ~\n        if (is.null(.x)) tibble()\n        else if (is.data.frame(.x)) as_tibble(.x)\n        else tibble::as_tibble(.x)\n      )\n    ) |&gt;\n    # 2) One row per author record (keeps pubs with 0 authors via keep_empty)\n    unnest(authorships, keep_empty = TRUE, names_sep = \"_\") |&gt;\n    # 3) Make sure each element of `affiliations` is a tibble (handles NULLs/lists)\n    mutate(\n      authorships_affiliations = map(authorships_affiliations, ~\n        if (is.null(.x)) tibble()\n        else if (is.data.frame(.x)) as_tibble(.x)\n        else tibble::as_tibble(.x)\n      )\n    ) |&gt;\n    # 4) One row per author record (keeps pubs with 0 authors via keep_empty)\n    unnest(authorships_affiliations, keep_empty = TRUE, names_sep = \"_\")  |&gt;\n    # 3) Make sure each element of `affiliations` is a tibble (handles NULLs/lists)\n    mutate(\n      last_known_institutions = map(last_known_institutions, ~\n        if (is.null(.x)) tibble()\n        else if (is.data.frame(.x)) as_tibble(.x)\n        else tibble::as_tibble(.x)\n      )\n    ) |&gt;\n    # 4) One row per author record (keeps pubs with 0 authors via keep_empty)\n    unnest(last_known_institutions, keep_empty = TRUE, names_sep = \"_\")\n}\n\n# pubs_authors &lt;- unnest_authors(pubs, dems) |&gt;\n#   filter(author_id == authorships_id) |&gt;\n#   filter(authorhships_affiliations)\n\n\npubs_authors = pubs |&gt;\n  left_join(dems, by = join_by(author_id == id)) |&gt;\n  unnest_authors()|&gt;\n  filter(author_id == authorships_id) |&gt;\n  filter(\n    is.na(authorships_affiliations_id) |\n    authorships_affiliations_id == last_known_institutions_id\n  )\n\n\nuniversities = c(\n    \"https://openalex.org/I865915315\", \"https://openalex.org/I887064364\",\n    \"https://openalex.org/I121797337\", \"https://openalex.org/I145872427\", \n    \"https://openalex.org/I193662353\", \"https://openalex.org/I913958620\", \n    \"https://openalex.org/I193700539\", \"https://openalex.org/I169381384\", \n    \"https://openalex.org/I913481162\"\n)\n\ndates = pubs_authors |&gt;\n    distinct(\n      authorships_display_name, authorships_id, authorships_affiliations_id, \n      authorships_affiliations_display_name, publication_date,\n      .keep_all=TRUE\n    ) |&gt;\n    filter(authorships_affiliations_id %in% universities) |&gt;\n    arrange(publication_date) |&gt;\n    group_by(authorships_id, authorships_affiliations_display_name, authorships_affiliations_id) |&gt;\n    summarize(\n        authorships_display_name = first(authorships_display_name),\n        first_date = first(publication_date) ,\n        last_date = last(publication_date)\n    ) |&gt;\n    mutate(\n      first_date = first_date %m-% months(18),\n      last_date = last_date %m+% months(6)\n    )\n\n\ntimepoints &lt;- c(\"2019-09-01\", \"2022-12-19\", \"2024-04-19\", \"2025-10-02\")\ntp &lt;- ymd(timepoints)\n\nd = dates |&gt;\n  tidyr::expand_grid(timepoint = tp) |&gt;\n  arrange(timepoint) |&gt;\n  mutate(\n    in_range = between(timepoint, first_date, last_date),\n    year = as.integer(year(timepoint) - 2000) \n  ) |&gt;\n  filter(in_range) |&gt;\n  distinct(authorships_display_name, authorships_id, authorships_display_name, authorships_affiliations_display_name, year) |&gt;\n  pivot_wider(\n    id_cols = c(authorships_display_name, authorships_id),\n    names_from = year,\n    names_prefix = 'affiliation_',\n    values_from = authorships_affiliations_display_name,\n    values_fn = list(authorships_affiliations_display_name = \\(x) list(sort(unique(na.omit(x)))))\n  ) |&gt;\n  arrange(authorships_display_name)\n\nfsaveRDS(d, 'institutions_patch')"
  },
  {
    "objectID": "src/docs/content/scrape_openalex.html",
    "href": "src/docs/content/scrape_openalex.html",
    "title": "Data Preparation OpenAlex-ID",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(openalexR)\nlibrary(rvest)\nlibrary(jsonlite)\nlibrary(cli)\nlibrary(stringi)\nlibrary(stringdist)\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()\n\noptions(openalexR.mailto = \"jos.slabbekoorn@ru.nl\")"
  },
  {
    "objectID": "src/docs/content/scrape_openalex.html#getting-started",
    "href": "src/docs/content/scrape_openalex.html#getting-started",
    "title": "Data Preparation OpenAlex-ID",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(openalexR)\nlibrary(rvest)\nlibrary(jsonlite)\nlibrary(cli)\nlibrary(stringi)\nlibrary(stringdist)\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()\n\noptions(openalexR.mailto = \"jos.slabbekoorn@ru.nl\")"
  },
  {
    "objectID": "src/docs/content/scrape_openalex.html#functions",
    "href": "src/docs/content/scrape_openalex.html#functions",
    "title": "Data Preparation OpenAlex-ID",
    "section": "Functions",
    "text": "Functions\n\nnormalize_university_names = function(df){\n  # 1) Pattern → c(english, native) (use ^...$ to force exact matches)\n   lookup = list(\n    \"VU\"                = c(\"Free University Amsterdam\", \n                            \"Vrije Universiteit Amsterdam\"),\n    \"(UvA|Uva)\"         = c(\"University of Amsterdam\", \n                            \"Universiteit van Amsterdam\"),\n    \"Leiden\"            = c(\"Leiden University\",\n                            \"Leiden Universiteit\"),\n    # this pattern matches with “RU”, “(RU)”, “RU,”, etc., and excludes “RUG”.\n    \"\\\\bRU\\\\b\"          = c(\"Radboud University Nijmegen\",\n                            \"Radboud Universiteit Nijmegen\"),\n    \"(UU|UCU)\"          = c(\"Utrecht University\", \n                            \"Universiteit Utrecht\"),\n    \"^EUR$\"             = c(\"Erasmus University Rotterdam\", \n                            \"Erasmus Universiteit Rotterdam\"),\n    \"(RUG|UvG)\"         = c(\"University of Groningen\", \n                            \"Rijksuniversiteit Groningen\"),\n    \"(UvT|Uvt|Tilburg)\" = c(\"University of Tilburg\", \n                            \"Universiteit van Tilburg\"),\n    \"WUR\"               = c(\"Wageningen University & Research\", \n                            \"Wageningen University & Research\"),\n    \"^TU Delft$\"        = c(\"Technical University Delft\", \n                            \"Technische Universiteit Delft\"),\n    \"Milano\"            = c(\"University of Milan\", \n                            \"Università degli Studi di Milano Statale\"),\n    \"Berlijn\"           = c(\"University of Hamburg\",\n                            \"Universität Hamburg\"),\n    \"Trento\"            = c(\"University of Trento\",\n                            \"Università degli Studi di Trento\"),\n    \"Stockholm\"         = c(\"Stockholm University\", \n                            \"Stockholms Universitet\"),\n    \"Gent\"              = c(\"Ghent University\", \n                            \"Universiteit Gent\"),\n    # Only native names (leave EN as-is)\n    \"Gothenburg\"        = c(NA, \"Göteborgs universitet\"),\n    \"Cologne\"           = c(NA, \"Universität zu Köln\"),\n    \"Koc\"               = c(NA, \"Koç Üniversitesi\"),\n    \"Turku\"             = c(NA, \"Turun Yliopisto\"),\n    \"Lausane\"           = c(NA, \"Université de Lausanne\"),\n    \"Leipzig\"           = c(NA, \"Universität Leipzig\"),\n    \"Linköping\"         = c(NA, \"Linköpings universitet\"),\n    # Explicit NAs\n    \"^Politie$\"         = c(NA, NA),\n    \"^UvH$\"             = c(NA, NA)\n  )\n\n  # 2) set the default values for university_names\n  df = df |&gt;\n    mutate(\n      university = map_chr(university, ~ {\n          if (is.null(.x) || length(.x) == 0) return(NA_character_)\n          as.character(.x[[1]])\n        }),\n      university       = str_replace(university, \"(Uni |uni)\", \"University \"),\n      university_name  = university,             # default EN → original\n      university_name2 = NA_character_           # default native → NA\n    )\n\n  # 3) replace the university name when the pattern is detected in university\n  for (pat in names(lookup)) {\n    vals = lookup[[pat]]\n    idx  = str_detect(df$university, pat)\n\n    if (!is.na(vals[1])) {\n      df$university_name[idx & df$university_name == df$university] = vals[1]\n    }\n    if (!is.na(vals[2])) {\n      df$university_name2[idx & is.na(df$university_name2)] = vals[2]\n    }\n  }\n\n  # 4) stack the values of the two university_name variables\n  df = df |&gt;\n    pivot_longer(\n      cols = c(university_name, university_name2),\n      names_to = \"var\",\n      values_to = \"university_name\"\n    ) |&gt;\n    select(uid, naam, clean_name, university, university_name)\n\n  return(df)\n}\n\n\noa_fetch_institutions = function(institutions, pause=0){\n  k = length(institutions)\n  hold = list()\n\n  cli_alert(\"Starting now, at {Sys.time()}\")\n  cli_progress_bar(\"Scraping institutions\", total = k, clear = FALSE)\n  # 1) iterate over all institutionsinstitutions\n  for (institution in institutions){\n    # sleep if a pause time is set\n    if (pause &gt; 0) Sys.sleep(pause)\n\n    # 2) fetch result for all institutions\n    res = tryCatch(\n      oa_fetch(\n          entity = \"institutions\", search=institution\n        ),\n      error = function(e) NULL,\n      warning = function(w) NULL\n    )\n\n    cli_progress_update()\n    \n\n    # 3) for cases with a valid result rename colums\n    if (is.null(res) || !is_tibble(res)) next\n    res = res |&gt; rename(\n      \"institution_id\" = \"id\",\n      \"institution_name\" = \"display_name\"\n    )\n\n    # 4.1) single result hitsare directly stored\n    if (nrow(res) == 1L) {\n      hold[[institution]] &lt;- res\n      next\n    }\n\n    # 4.2) multiple result hits are processed and checked the institution_name \n    #      matches the OpenAlex display_name for institutions\n    pat = paste0(\"^\", institution, \"$\")\n    idx = str_detect(res$institution_name, pat)\n\n    if (sum(idx, na.rm = TRUE) == 0L) {\n      hold[[institution]] = res\n      next\n    }\n\n    if (sum(idx, na.rm = TRUE) == 1L) {\n      hold[[institution]] = res[idx,]\n      next\n    }\n\n    # if multiple matches exist with only select the first (most relevant) one\n    if (sum(idx, na.rm = TRUE) &gt; 1L) {\n      hold[[institution]] = res[idx,][1, ]\n    }\n  }\n\n  # combine the fetched data and harmonize this data\n  out = bind_rows(hold) |&gt; \n    distinct(institution_id, .keep_all=TRUE) |&gt;\n    select(institution_name, institution_id) |&gt;\n    mutate(\n      institution_url = institution_id,\n      institution_id = str_remove(institution_id, 'https://openalex.org/')\n    )\n\n  return(out)\n}\n\n\nadd_institution_id = function(df){\n  # create a vector all the unique university names\n  institutions = df$university_name |&gt; unique() |&gt; na.omit()\n  institutions = oa_fetch_institutions(institutions)\n\n  # merge the institutions data with df and harmonize the data\n  out = df |&gt;\n    rename(\n      \"institution_name\" = \"university_name\",\n      \"institution\" = \"university\"\n    ) |&gt;\n    left_join(institutions) |&gt;\n    filter(!is.na(institution_id)) |&gt;\n    distinct(uid, institution_id, .keep_all=TRUE)\n\n  return(out)\n}\n\n\nnormalize_name = function(x) {\n  x = as.character(x)\n\n  # mark Cyrillic\n  has_cyr = stri_detect_charclass(x, \"\\\\p{Script=Cyrillic}\")\n\n  # transliterate only Cyrillic -&gt; Latin\n  x[has_cyr] = stri_trans_general(x[has_cyr], \"Cyrillic-Latin\")\n\n  # strip accents for all, lowercase, trim, squish\n  x = stri_trans_general(x, \"Latin-ASCII\")\n  x = tolower(x)\n  x = str_squish(x)\n  trimws(x)\n}\n\nadd_query_similarity = function(data){\n  data |&gt;\n    mutate(\n      dn = normalize_name(display_name),\n      qn = normalize_name(query_name),\n      query_similarity   = 1 - stringdist(dn, qn, method = \"jw\")\n    ) |&gt;  # Jaro–Winkler\n    select(-dn, -qn)\n}\n\n\noa_fetch_authors = function(df, pause = 0){\n  # configure iterator and other looping variables\n  uids = df$uid |&gt; unique() |&gt; na.omit()\n  k = length(uids)\n  hold = list()\n\n  cli_alert(\"Starting now, at {Sys.time()}\")\n  cli_progress_bar(\"Scraping authors\", total = k, clear = FALSE)\n  for (id in uids) {\n    # extract information for each author\n    tab = df |&gt; filter(uid ==  id)\n    if (nrow(tab) == 0) stop(\"invalid result:\\n\", tab)\n\n    # pull the required information\n    clean_name = tab |&gt; distinct(clean_name) |&gt; pull()\n    institution_ids = tab |&gt; distinct(institution_id) |&gt; pull()\n\n    if (length(institution_ids) == 0L) next \n    # skip cases without any institutions\n    # TODO: rework code to include cases without institution.\n    # TODO: implement fetch for cases without institutions\n\n    res = tryCatch(\n      oa_fetch(\n        entity = 'author',\n        search = clean_name,\n        affiliations.institution.id = institution_ids,\n      ),\n      error = function(e) NULL,\n      warning = function(w) NULL\n    )\n    cli_progress_update()\n\n    # skip institution if invalid result\n    if (is.null(res) || !is_tibble(res) || (nrow(res) == 0)) next\n\n    # harmonize and expand the data\n    hold[[id]] = res |&gt; rename('author_id' = 'id') |&gt;\n      distinct(author_id, .keep_all = TRUE) |&gt;\n      mutate(\n        uid = id,\n        query_name = clean_name,\n        .before = 1\n      ) |&gt;\n      add_query_similarity() |&gt;\n      arrange(-relevance_score)\n    \n  }\n  return(hold)\n}\n\n\nunnest_institutions = function(data){\n  hold = data\n  for (id in names(data)) {\n    # extract information for each author and skip if invalid\n    tab = data[[id]]\n    if (is.null(tab) || !is_tibble(tab) || !(nrow(tab) &gt; 0)) next\n\n    # clean authors data and unnests institution information\n    hold[[id]] = tab |&gt;\n      filter(uid == id) |&gt;\n      distinct(uid, author_id, .keep_all=TRUE) |&gt;\n      mutate(\n        # unpacks the dataframes found in last_known_institutions\n        last_known_institutions = map(\n          last_known_institutions,\n          ~ .x |&gt;\n            distinct(.keep_all = TRUE) |&gt;\n            mutate(\n              institution_id   = str_remove(id, \"^https://openalex.org/\"),\n              institution_name = display_name\n            ) |&gt;\n            select(id, institution_id, institution_name, country_code)\n        ),\n        # unnests the institution_ids\n        institution_ids = map(\n          last_known_institutions, \n          ~ .x |&gt; pull(institution_id) |&gt; unique()\n        )    \n      )\n  }\n  return(hold)\n}\n\n\nmatch_institutions = function(data, df){\n  hold = data\n  for(id in names(data)){\n    # extract information for each author and skip if invalid\n    tab = data[[id]]\n    if (is.null(tab) || !is_tibble(tab) || !(nrow(tab) &gt; 0)) next\n\n    # create a list of institution_ids per author and skip if invalid\n    institutions = df |&gt;\n      filter(uid == id) |&gt;\n      pull(institution_id) |&gt;\n      unique()\n    if ((length(institutions) &lt; 1)) next\n\n    #... \n    hold[[id]] = tab |&gt;\n      mutate(\n        # indicates whether there is a match in institution_id between OpenAlex\n        # and the original dataframe\n        institution_match = map_lgl(\n          institution_ids, \n          ~ {x = .x; if (is.null(x)) x = character(0)\n            any(as.character(x) %in% institutions)}\n          ),\n        keep = case_when(\n            query_similarity == 1.0 ~ TRUE,\n            !institution_match & query_similarity &gt;= 0.60 ~ TRUE,\n            institution_match & query_similarity &gt;= 0.45 ~ TRUE,\n            .default = FALSE\n          ),\n        keep = case_when(\n            display_name == 'Thijs W. de Vos' ~ FALSE,\n            display_name == 'Kees van Kersbergen' ~ FALSE,\n            display_name == 'René W. van der Hulst' ~ FALSE,\n            display_name == 'Younes Zoughlami' ~ FALSE,\n            display_name == 'Younes Zeboudj' ~ FALSE,\n            display_name == 'Younes Saramifar' ~ FALSE,\n            display_name == 'van Dijk' ~ FALSE,\n            display_name == 'Stephanie Maas' ~ FALSE,\n            display_name == 'A.J.J. Nijhuis' ~ FALSE,\n            (query_name == 'Meindert Fennema') \n              & (display_name == 'Wouter van der Brug') ~ FALSE,\n            .default = keep\n          ),\n        orcid = as.character(orcid)\n      ) |&gt;\n      filter(keep)\n  }\n  return(hold)\n}\n\n\noa_fetch_works = function(data, pause = 0){\n  # configure iterator and other looping variables\n  k = length(data)\n  hold = list()\n\n  cli_alert(\"Starting now, at {Sys.time()}\")\n  cli_progress_bar(\"Scraping works\", total = k, clear = FALSE)\n  for (id in names(data)){\n    cli_progress_update()\n    # extract information for each author and skip if invalid\n    tab = data[[id]]\n    if (any(!is_tibble(tab), is.null(tab), (nrow(tab) &lt; 1))) next\n    if (nrow(tab) &lt; 1) next\n\n    # extract a list of institution_ids \n    oa_ids = tab |&gt; \n      mutate(author_id = str_remove(author_id, 'https://openalex.org/')) |&gt;\n      distinct(author_id) |&gt; pull()\n    if (any(is.null(oa_ids) || (length(oa_ids) &lt; 1))) next\n\n    subhold = list()\n    for (oa_id in oa_ids){\n      res = tryCatch(\n        oa_fetch(\n            entity = 'works',\n            author.id = oa_id\n        ),\n        error = function(e) NULL,\n        warning = function(w) NULL\n      )\n\n      if (any(!is_tibble(res), is.null(res), (nrow(res) &lt; 1))) next\n\n      subhold[[oa_id]] = res |&gt;\n        rename(\"work_id\" = \"id\") |&gt;\n        mutate(\n          uid = id,\n          author_id = oa_id,\n          .before = 1\n        ) \n    }\n\n    if (length(subhold) == 0) {\n      hold[[id]] = tibble()\n      next\n    }\n\n    hold[[id]] = bind_rows(subhold) |&gt;\n      distinct(work_id, .keep_all = TRUE) |&gt;\n      filter(uid == id)\n    \n  }\n  return(hold)\n}"
  },
  {
    "objectID": "src/docs/content/scrape_openalex.html#application",
    "href": "src/docs/content/scrape_openalex.html#application",
    "title": "Data Preparation OpenAlex-ID",
    "section": "Application",
    "text": "Application\n\n# load the most recent names data\ndir = file.path('data', 'processed')\nfile = list.files(dir, pattern = 'names.Rds')\nnames = freadRDS(file.path(dir, file[length(file)])) \n\n# load the most resent scholarid data\ndir = file.path('data', 'processed')\nfile = list.files(dir, pattern = 'scholarid.Rds')\nids = freadRDS(file.path(dir, file[length(file)])) \n\n# Combine names and scholarid data and reshape to create a table with\n# a row per author university pair\ndf = ids |&gt;\n  left_join(names) |&gt;\n  distinct(uid, university, .keep_all = TRUE) |&gt;\n  unnest_longer(university) |&gt;\n  normalize_university_names() |&gt;\n  add_institution_id()\n\n\n# create a authors table\nauthors = df |&gt;\n  oa_fetch_authors() |&gt; \n  unnest_institutions() |&gt;\n  match_institutions(df)\n\nfsaveRDS(authors, 'oaauthors', location = \"./data/raw_data/\")\n\n\na = bind_rows(authors)\na |&gt; head()\n\n\n# create a works table\nworks = oa_fetch_works(authors)\nfsaveRDS(works, 'oaworks', location = \"./data/raw_data/\")\n\n\nw = bind_rows(works)\nw |&gt; head()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scientific Co-Publishing Networks",
    "section": "",
    "text": "Note\n\n\n\nProject Description goes here"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Scientific Co-Publishing Networks",
    "section": "",
    "text": "Note\n\n\n\nProject Description goes here"
  },
  {
    "objectID": "index.html#manuscript",
    "href": "index.html#manuscript",
    "title": "Scientific Co-Publishing Networks",
    "section": "Manuscript",
    "text": "Manuscript\n\n\n\n\n\n\nNote\n\n\n\nA reference to the manuscript pdf goes here."
  },
  {
    "objectID": "src/docs/content/create_scholars.html",
    "href": "src/docs/content/create_scholars.html",
    "title": "Data Processing Scholars",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  605293 32.4    1371507 73.3         NA   715802 38.3\nVcells 1122109  8.6    8388608 64.0      16384  2012236 15.4\n\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(renv)\nlibrary(stringr)\nlibrary(janitor)\nlibrary(lubridate)\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()"
  },
  {
    "objectID": "src/docs/content/create_scholars.html#getting-started",
    "href": "src/docs/content/create_scholars.html#getting-started",
    "title": "Data Processing Scholars",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  605293 32.4    1371507 73.3         NA   715802 38.3\nVcells 1122109  8.6    8388608 64.0      16384  2012236 15.4\n\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(renv)\nlibrary(stringr)\nlibrary(janitor)\nlibrary(lubridate)\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()"
  },
  {
    "objectID": "src/docs/content/create_scholars.html#functions",
    "href": "src/docs/content/create_scholars.html#functions",
    "title": "Data Processing Scholars",
    "section": "Functions",
    "text": "Functions\n\nLoading in Raw Data\nThis function loads in Excel files containing information on scholars working in Dutch sociology and political science departments. The data have been collected at three points in time — 19 December 2022, 19 April 2024, and 1 October 2025 — to capture changes in staff composition over time. Each Excel file includes two sheets, one for Sociologie and one for Politicologie, which are read in separately, harmonized, and then concatenated into a single dataset that combines all disciplines and time points.\n\n# define tailored functions\nread_spreadsheet = function(filepath, sheet){  \n  readxl::read_excel(filepath, sheet = sheet) \n}\n\nharmonize_columns = function(data, discipline, date){\n  data = data |&gt;\n    clean_names() |&gt;\n    mutate(\n      discipline = discipline,\n      date = date\n    ) |&gt;\n    relocate(c(discipline, date), .before = 1)\n}\n\nfread = function(\n    files, \n    source,\n    disciplines = c( \"Politicologie\", \"Sociologie\"),\n    language = 'nl'\n  ){\n  # read in both sheets from excel files\n  hold = c()\n  for (file in files){\n    filepath = file.path(source, file)\n    date = ymd(str_split(file, pattern = \"_\")[[1]][1])\n\n    # each file contains two sheets, for each discipline\n    # read in both sheets and combine these data\n    disciplines = c( \"Politicologie\", \"Sociologie\")\n    chunks = c()\n    for (discipline in disciplines){\n      chunk = read_spreadsheet(filepath, discipline) |&gt;\n        harmonize_columns(discipline, date)\n\n      chunks[[discipline]] = chunk\n    }\n\n    hold[[file]] =  bind_rows(chunks)\n  }\n\n  data = bind_rows(hold)\n\n  if (!language %in% c('nl', 'en')) stop(\"language should be nl or en\")\n  # implement rename function for english column namesac\n  \n\n  return(data)\n}\n\n\n\nFix Scholar Names\nThis step standardizes scholar names to ensure consistent matching across sources. It first normalizes name particles common in Dutch and related languages (e.g., van, de, der, ten, ’t, le, el, op den) by converting them to lowercase when they appear between spaces (e.g., “Jan van Dijk”). Next, it corrects a curated set of frequent typos and formatting inconsistencies - such as fixing misplaced or missing initials, diacritics, and misspellings (e.g., “AJGM van Montfort” to “A.J.G.M. van Montfort,” “Lea Kroner” to “Lea Kröner”). The result is a cleaned naam field with harmonized capitalization and corrected names, improving join accuracy and downstream deduplication.\n\nfix_scholar_names = function(data){\n  # fix the particles in the names\n  particles = c(\n    \"van\", \"in\",\n    \"de\", \"den\", \"der\", \"den\", \"del\",\n    \"te\", \"ten\", \"ter\", \"tes\", \"'t\",\n    \"la\", \"le\", \"les\", \"los\", \"el\", \"el-\",\n    \"op den\"\n  )\n  pattern = regex(\n    paste0(\"(?&lt;=\\\\s)(\", paste(particles, collapse = \"|\"), \")(?=\\\\s)\"),\n    ignore_case = TRUE\n  )\n\n  data |&gt;\n    mutate(\n      # replace capitalized particles\n      naam = str_replace_all(naam, pattern, ~ tolower(.x)),\n\n      # replace typos and mistakes in names\n      naam = case_when(\n        naam == \"Andrea Forstrer\"      ~ \"Andrea Forster\",\n        naam == \"AJGM van Montfort\"    ~ \"A.J.G.M. van Montfort\",\n        naam == \"FP Wagenaar\"          ~ \"F.P. Wagenaar\",\n        naam == \"JP Presley\"           ~ \"J.P. Presley\",\n        naam == \"JS Timmer\"            ~ \"J.S. Timmer\",\n        naam == \"ilya Lavrov\"          ~ \"Ilya Lavrov\",\n        naam == \"p. Vila Soler\"        ~ \"P. Vila Soler\",\n        naam == \"Z Dong\"               ~ \"Z. Dong\",\n        naam == \"Renae  Loh\"           ~ \"Renae Loh\",\n        naam == \"Paulina Pankowski\"    ~ \"Paulina Pankowska\",\n        naam == \"M.M Cuperus\"          ~ \"M.M. Cuperus\",\n        naam == \"Lea Kroner\"           ~ \"Lea Kröner\",\n        naam == \"L Slot\"               ~ \"L. Slot\",\n        naam == \"Jan Willen Duyvendak\" ~ \"Jan Willem Duyvendak\",\n        .default = naam\n      )\n    )\n}\n\n\n\nFixing Google Scholar ID\nThis step propagates known Google Scholar IDs across time for the same person. The data are first ordered by naam and date, then grouped by naam so that each individual’s records form a sequence. Within each group, missing values in google_scholar_id are filled using the nearest available value, with the fill direction controlled by the .direction argument (default “updown” fills forward and backward; alternatives like “down” or “up” restrict the fill to one direction). Groups are then ungrouped and the dataset is tidied by universiteit and date. This reduces missing IDs while ensuring values never leak across different people; it assumes that identical names refer to the same scholar, so remaining homonyms should be checked upstream.\n\nfix_google_scholar_id = function(data, .direction='updown'){\n  data |&gt;\n    arrange(naam, date) |&gt;\n    group_by(naam) |&gt;\n    # fill missing values with available information\n    fill(google_scholar_id, .direction = .direction) |&gt;  \n    ungroup() |&gt;\n    arrange(universiteit, date)\n}\n\n\n\nFixing Email Addresses\nThis step standardizes and completes the email address information for each scholar. It first applies a practical, case-insensitive regular expression that captures valid email formats, including subdomains, to extract clean addresses from the email_adres field. All extracted emails are then converted to lowercase to ensure consistency. Next, the data are grouped by universiteit and naam, and missing email values are filled using the nearest available information within each group (by default in both directions, controlled by the .direction argument). Finally, the dataset is ungrouped and ordered by universiteit and date, resulting in a harmonized and more complete set of email addresses that align across time points for the same scholar\n\nfix_email_adresses = function(data, .direction = \"updown\"){\n  # practical email regex (case-insensitive), supports subdomains\n  email_pattern = regex(\n    \"\\\\b[[:alnum:]._%+-]+@[[:alnum:]-]+(?:\\\\.[[:alnum:]-]+)+\\\\b\", \n    ignore_case = TRUE\n  )\n\n  # clean email variable\n  data |&gt;\n    mutate(\n      email_adres = str_extract(\n        email_adres, email_pattern\n        ) |&gt; tolower()\n    ) |&gt;\n    group_by(universiteit, naam) |&gt;\n    # fill \n    fill(email_adres, .direction = \"updown\") |&gt;\n    ungroup()|&gt;\n    arrange(universiteit, date)\n}\n\n\n\nFixing Universities\nThis step harmonizes university affiliations and adds a canonical code alongside the raw label. It first splits multi-valued entries in universiteit (e.g., “UU / UvA”) into separate rows, then trims and normalizes each label. Using a case-insensitive pattern, it maps recognized names/abbreviations to a standard set (EUR, RU, RUG, UU, VU, UvA, UvT, Leiden). If a canonical code is still missing, it infers the affiliation from the email domain (e.g., …@essb.eur.nl -&gt; EUR, …@vu.nl -&gt; VU, …@uva.nl -&gt; UvA, etc.). After removing duplicates, affiliations are re-aggregated per person and date into a list column university, joined back to the original data, and positioned next to universiteit. The result is a consistent, machine-readable university code that supports reliable grouping, filtering, and longitudinal comparison.\n\nclean_universities = function(data){\n  universities = c(\"EUR\", \"RU\", \"RUG\", \"UU\", \"VU\", \"UvA\", \"UvT\", \"LU\")\n  pat   = regex(\"\\\\b(EUR|RU|RUG|UU|VU|UvA|UvT|Leiden)\\\\b\", ignore_case = TRUE)\n  canon = setNames(universities, universities)\n\n  # clean universities\n  uni = data |&gt;\n    # split university strings on '\\s' , '/', '\\.', and '?'\n    mutate(\n      universiteit = str_split(universiteit, \"/+|\\\\?+|\\\\.\"),\n    ) |&gt;\n    unnest_longer(universiteit) |&gt;\n    # clean the university labels\n    mutate(\n      universiteit = str_replace(\n        str_squish(universiteit), 'Leiden uni', 'Leiden'\n      ),\n      university = str_replace(universiteit, pat, \\(m) canon[str_to_lower(m)]),\n      universiteit = case_when(\n        (is.na(university) & str_detect(email_adres, 'essb.eur.nl')) ~ 'EUR',\n        (is.na(university) & str_detect(email_adres, 'vu.nl'))  ~ 'VU',\n        (is.na(university) & str_detect(email_adres, 'uva.nl'))  ~  'UVA',\n        (is.na(university) & str_detect(email_adres, 'leidenuni'))  ~  'Leiden',\n        (is.na(university) & str_detect(email_adres, 'ru.nl'))  ~  'RU',\n        (is.na(university) & str_detect(email_adres, 'rug.nl'))  ~  'RUG',\n        (is.na(university) & str_detect(email_adres, 'tilburguni'))  ~  'UvT',\n        (is.na(university) & str_detect(email_adres, 'uu.nl'))  ~  'UU',\n        .default = university\n      ),\n      university = ifelse(\"\" == university, NA_character_, university)\n    ) |&gt;\n    distinct(.keep_all=TRUE) |&gt;\n    group_by(naam, date) |&gt; \n    summarise(university = list(unlist(university))) |&gt;\n    ungroup()\n\n  data |&gt;\n    left_join(uni) |&gt;\n    relocate(university, .after=universiteit)\n}\n\n\n\nClean functie\nThis step parses free-text job titles in functie into a set of consistent role flags. It first keeps only date, naam, and functie, then lowercases functie for keyword matching. Using targeted patterns in Dutch and English, it creates boolean indicators for common categories:\n\nvisiting (e.g., gast, visit),\nexternal (buiten, external),\nprofessor (incl. typos like proffessor),\nassociate professor (hoofddocent, associate, uhd),\nassistant professor (universitair docent, assistant),\npostdoc (postdoc, doctoral),\nlecturer (docent, lecturer, teacher),\nresearcher (onderzoeker, research),\nPhD (phd, promovend),\nsenior/junior,\nemeritus (professor + emiri) or endowed (professor + bijzon),\nfellow,\nand a broad staff (e.g., advisor, secretary, assistent, medewerker, manager, coordinator, director/directeur).\n\nTo avoid double counting, several flags are explicitly set to FALSE when a more specific academic rank applies (e.g., assistant/associate/full professor precedence over lecturer/researcher/staff). Missing titles propagate as NA in the corresponding flags. The result is a tidy, machine-readable set of role indicators that standardizes heterogeneous job titles for downstream classification and analysis.\n\nparse_job_titles = function(data){\n  data |&gt;\n    select(date, naam, functie) |&gt;\n    mutate(\n      is_visiting = case_when(\n        str_detect(str_to_lower(functie), 'gast') ~ TRUE,\n        str_detect(str_to_lower(functie), 'visit') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_external = case_when(\n        str_detect(str_to_lower(functie), 'external') ~ TRUE,\n        str_detect(str_to_lower(functie), 'buiten') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_associate_professor = case_when(\n        str_detect(str_to_lower(functie), 'hoofddocent') ~ TRUE,\n        str_detect(str_to_lower(functie), 'associate ') ~ TRUE,\n        str_detect(str_to_lower(functie), 'uhd') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_assistant_professor = case_when(\n        is_associate_professor ~ FALSE,\n        str_detect(str_to_lower(functie), 'universitair docent') ~ TRUE,\n        str_detect(str_to_lower(functie), 'assistant ') ~ TRUE, \n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_postdoc = case_when(\n        str_detect(str_to_lower(functie), 'postdoc') ~ TRUE,\n        str_detect(str_to_lower(functie), 'doctoral') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_senior = case_when(\n        str_detect(str_to_lower(functie), 'senior') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_junior = case_when(\n        str_detect(str_to_lower(functie), 'junior') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_lecturer = case_when(\n        is_associate_professor ~ FALSE, \n        is_assistant_professor ~ FALSE, \n        str_detect(str_to_lower(functie), 'lecturer') ~ TRUE,\n        str_detect(str_to_lower(functie), 'docent') ~ TRUE,\n        str_detect(str_to_lower(functie), 'teacher') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_researcher = case_when(\n        is_associate_professor ~ FALSE, \n        is_assistant_professor ~ FALSE,\n        is_postdoc ~ FALSE, \n        str_detect(str_to_lower(functie), 'onderzoeker') ~ TRUE,\n        str_detect(str_to_lower(functie), 'research') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_phd = case_when(\n        str_detect(str_to_lower(functie), 'phd') ~ TRUE,\n        str_detect(str_to_lower(functie), 'promovend') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_professor = case_when(\n        is_associate_professor ~ FALSE, \n        is_assistant_professor ~ FALSE,\n        is_postdoc ~ FALSE, \n        str_detect(str_to_lower(functie), 'hoogleraar') ~ TRUE,\n        str_detect(str_to_lower(functie), 'professor') ~ TRUE,\n        str_detect(str_to_lower(functie), 'proffessor') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_emeritus = case_when(\n        is_professor & str_detect(str_to_lower(functie), 'emiri') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_endowed = case_when(\n        is_professor & str_detect(str_to_lower(functie), 'bijzon') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_staff = case_when(\n        # make sure that people with other positions are not falsely\n        # been configured to be a staff member.\n        is_associate_professor ~ FALSE, \n        is_assistant_professor ~ FALSE,\n        is_lecturer ~ FALSE,\n        is_postdoc ~ FALSE,\n        is_professor ~ FALSE,\n        # staff members have wildly varying job titles.\n        str_detect(str_to_lower(functie), 'advisor') ~ TRUE,\n        str_detect(str_to_lower(functie), 'secretary') ~ TRUE,\n        str_detect(str_to_lower(functie), 'assistent') ~ TRUE,\n        str_detect(str_to_lower(functie), 'medewerk') ~ TRUE,\n        str_detect(str_to_lower(functie), 'market') ~ TRUE,\n        str_detect(str_to_lower(functie), 'managing') ~ TRUE,\n        str_detect(str_to_lower(functie), 'manager') ~ TRUE,\n        str_detect(str_to_lower(functie), 'coordinator') ~ TRUE,\n        str_detect(str_to_lower(functie), 'director') ~ TRUE,\n        str_detect(str_to_lower(functie), 'directeur') ~ TRUE,\n      ),\n      is_fellow = case_when(\n        str_detect(str_to_lower(functie), 'fellow') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      )\n    )\n}\n\nBuilding on the role flags derived in Clean functie, this step converts those boolean indicators into human-readable position labels and, optionally, a compact string that also captures distinctions.\n\nconstruct_positions() maps the mutually exclusive academic roles to a canonical position (e.g., Full Professor, Associate Professor, Assistant Professor, Postdoctoral Researcher, PhD Candidate, Lecturer, Researcher, Staff) while separately flagging distinctions as short text labels: Visiting, External, Senior, Junior, Emeritus, Endowed, and Fellow.\n\nAfter creating these columns, it drops the original is_ flags and constructs position2 by uniting any present distinctions (from visiting through fellow) into a single, comma-separated string (leaving the per-column distinction flags intact for auditing).\n\nclean_functie() then orchestrates the full cleaning. It first runs parse_job_titles() to produce the role flags, passes the result through construct_positions(), and finally writes the output back into the original data:\n.what = \"complete\" (default) sets functie to the canonical position (rank only).\n.what = \"simplified\" sets functie to position2, which includes any distinctions (e.g., Assistant Professor, Visiting; Fellow).\n\nThe result is a consistent functie column suitable either for strict rank analyses (complete) or for descriptive reporting that preserves visiting/external/fellow/etc. qualifiers (simplified).\n\nconstruct_positions = function(data) {\n  data |&gt;\n    mutate(\n      # make flags for people with one of the following distinctions\n      visiting = ifelse(is_visiting, 'Visiting', NA_character_),\n      external = ifelse(is_external, 'External', NA_character_),\n      senior = ifelse(is_senior, 'Senior', NA_character_),\n      junior = ifelse(is_junior, 'Junior', NA_character_),\n      emeritus = ifelse(is_emeritus, 'Emeritus', NA_character_),\n      endowed = ifelse(is_endowed, 'Endowed', NA_character_),\n      # create a basic positions variable, excluding distinctions\n      position = case_when(\n        is_professor ~ \"Full Professor\",\n        is_associate_professor ~ \"Associate Professor\",\n        is_assistant_professor ~ \"Assistant Professor\",\n        is_postdoc ~ \"Postdoctoral Researcher\",\n        is_phd ~ \"PhD Candidate\",\n        is_lecturer ~ \"Lecturer\",\n        is_researcher ~ \"Researcher\",\n        is_staff ~ \"Staff\",\n        .default = NA_character_\n      ),\n      fellow = ifelse(is_fellow, 'Fellow', NA_character_)\n    ) |&gt;\n    select(!starts_with('is_')) |&gt;\n    unite('position2', visiting:fellow, na.rm=TRUE, remove=FALSE)\n}\n\nclean_functie = function(data, .what='complete'){\n  test = data |&gt; \n    parse_job_titles() |&gt;\n    construct_positions()\n\n  if (.what == 'complete'){\n    data['functie'] = test$position\n  } else if (.what == 'simplified'){\n    data['functie'] = test$position2\n  }\n\n  return(data)\n}"
  },
  {
    "objectID": "src/docs/content/create_scholars.html#application",
    "href": "src/docs/content/create_scholars.html#application",
    "title": "Data Processing Scholars",
    "section": "Application",
    "text": "Application\n\n# identified scholarid files\nsource = file.path(\"data\", \"raw_data\")\nfiles = list.files(source, pattern = \"scholarid.xlsx\")\n\n# load and process data\ndata = fread(files, source) |&gt; \n  fix_scholar_names() |&gt;\n  clean_universities() |&gt;\n  fix_email_adresses() |&gt;\n  fix_google_scholar_id() |&gt;\n  select(-specialisatie, -notitie, -additional, -checked) |&gt;\n  arrange(discipline, date, naam, university) |&gt;\n  clean_functie()\n\n`summarise()` has grouped output by 'naam'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(date, naam)`\n\n# save data\nfsaveRDS(data, 'scholarid')\n\n[1] \"SAVING: ./data/processed/20251027scholarid.Rds\""
  },
  {
    "objectID": "src/docs/content/process_names.html",
    "href": "src/docs/content/process_names.html",
    "title": "Data Processing Names",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(sqids)\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()"
  },
  {
    "objectID": "src/docs/content/process_names.html#getting-started",
    "href": "src/docs/content/process_names.html#getting-started",
    "title": "Data Processing Names",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(sqids)\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()"
  },
  {
    "objectID": "src/docs/content/process_names.html#functions",
    "href": "src/docs/content/process_names.html#functions",
    "title": "Data Processing Names",
    "section": "Functions",
    "text": "Functions\n\nParse Names\n\nparse_names = function(names){\n  particles &lt;- c(\n    \"de\",\"den\",\"der\",\"het\",\"te\",\"ten\",\"ter\",\n    \"van\",\"van de\",\"van den\",\"van der\",\"van 't\",\"van ’t\",\n    \"'t\",\"’t\",\n    \"von\",\"von der\",\"von den\",\n    \"la\",\"le\",\"du\",\"del\",\"della\",\"di\",\"da\",\"dos\",\n    \"das\",\"de la\",\"de los\",\"de las\",\n    \"zu\",\"zum\",\"zur\", \"op de\"\n  )\n\n  # Normalization helper for matching\n  .normalize &lt;- function(x) {\n    x |&gt;\n      str_squish() |&gt;\n      str_replace_all(\"’\", \"'\") |&gt;\n      str_to_lower() |&gt;\n      stri_trans_general(\"Latin-ASCII\")\n    }\n\n  p_norm &lt;- .normalize(particles)\n\n  names |&gt;\n    mutate(\n      tokens_raw = str_split(naam, \"\\\\s+\"),\n      tokens_norm = map(tokens_raw, ~ .normalize(.x)),\n      n = map_int(tokens_raw, length),\n\n      # find the longest particle match immediately before the final token\n        particle_idx = map2_int(tokens_norm, n, function(tok, n_tok) {\n          if (n_tok &lt; 2) return(NA_integer_)\n          # check 3, 2, 1-token particles that end at position n_tok-1\n          for (k in 3:1) {\n            start &lt;- n_tok - k\n            end   &lt;- n_tok - 1\n            if (start &gt;= 1) {\n              cand &lt;- paste(tok[start:end], collapse = \" \")\n              if (cand %in% p_norm) return(start)\n            }\n          }\n          NA_integer_\n        }),\n\n        has_particle = !is.na(particle_idx),\n\n        first_name = pmap_chr(\n          list(tokens_raw, particle_idx, n),\n          function(tok, p_i, n_tok) {\n            end_giv &lt;- if (is.na(p_i)) n_tok - 1 else p_i - 1\n            if (end_giv &lt;= 0) tok[1] \n            else paste(tok[seq_len(end_giv)], collapse = \" \")\n          }\n        ),\n\n        particle = pmap_chr(\n            list(tokens_raw, tokens_norm, particle_idx, n),\n            function(tok_raw, tok_norm, p_i, n_tok) {\n              if (is.na(p_i)) return(NA_character_)\n              # output particle in lowercase, canonicalised apostrophes\n              out &lt;- paste(tok_norm[p_i:(n_tok-1)], collapse = \" \")\n              out\n            }\n          ) |&gt; str_to_lower(),\n\n        last_name = pmap_chr(\n          list(tokens_raw, n),\n          function(tok, n_tok) tok[n_tok]\n        )\n    ) |&gt;\n    select(naam, first_name, particle, last_name)\n}\n\n\n\nExtract Initials\n\nextract_initials = function(names){\n  names |&gt;\n    mutate(\n      initials = str_extract(\n          naam, \n          \"^((?:\\\\p{Lu}{1,2}\\\\.)+(?:-\\\\p{Lu}{1,2}\\\\.)*)(?=\\\\s)\"\n        ),\n      first_name = ifelse(is.na(initials), first_name, NA_character_)) |&gt;\n    relocate(initials, .after=naam)\n}\n\n\n\nPatch Names\n\npatch_names = function(names){\n  # read in dataset with corrections for name information\n  corrections = readRDS(file.path('data', 'utils', 'name_corrections.Rds'))\n\n  names |&gt; rows_update(corrections, by='naam', unmatched='ignore')\n}\n\n\nreadRDS(file.path('data', 'utils', 'name_corrections.Rds')) |&gt; head()\n\n\n  \n\n\n\n\n\nExtract Maiden Name\n\nextract_maiden_name = function(names){\n  # split last names\n  last_name_splits = str_split(names$last_name, '-', simplify = TRUE)\n\n  # add splits to names dataframe\n  names['last_name'] = last_name_splits[,1]\n  names['maiden_name'] = ifelse(\n    last_name_splits[,2] == '',\n    NA_character_,\n    last_name_splits[,2]\n  )\n\n  return(names)\n}\n\n\nformat_names = function(data){\n  data |&gt;\n    mutate(\n      initials = initials |&gt; stri_trans_general(\"Latin-ASCII\"),\n      first_name = first_name |&gt; stri_trans_general(\"Latin-ASCII\"),\n      particle = particle |&gt; \n        str_to_lower() |&gt;\n        stri_trans_general(\"Latin-ASCII\"),\n      last_name = last_name |&gt; stri_trans_general(\"Latin-ASCII\"),\n      maiden_name = maiden_name |&gt; stri_trans_general(\"Latin-ASCII\")\n    ) |&gt; \n    unite(\"clean_name\", \n          initials:last_name, na.rm=TRUE, sep=\" \", remove=FALSE) |&gt;\n    unite(\"clean_name_full\",\n          clean_name, maiden_name, na.rm=TRUE, sep=\"-\", remove=FALSE) |&gt;\n    mutate(\n      temp = str_remove(clean_name, paste0(initials, \" \")),\n      temp_full = str_remove(clean_name_full, paste0(initials, \" \")),\n      clean_name = case_when(\n        !is.na(initials) & !is.na(first_name) ~ temp,\n        .default=clean_name\n      ),\n      clean_name_full = case_when(\n        !is.na(initials) & !is.na(first_name) ~ temp_full,\n        .default=clean_name_full\n      ),\n    ) |&gt;\n    select(-starts_with(\"temp\"))\n    \n}\n\n\nadd_uid = function(names){\n  # configure sqids\n  settings &lt;- sqids::sqids_options(min_length = 10)\n  unique_names = names$clean_name |&gt; unique()\n\n  # generate a id for all names\n  lookup = list()\n  for (i in 1:length(unique_names)){\n    name = unique_names[[i]]\n    id = sqids::encode(i * 25, settings)\n    lookup[[i]] = tibble(\n      clean_name = name,\n      uid = id\n    )\n  }\n\n  lookup = bind_rows(lookup)\n  saveRDS(lookup, file.path('data', 'utils', 'uid_key.Rds'))\n\n  names |&gt;\n    left_join(lookup) |&gt;\n    relocate(uid, .before=1)\n}"
  },
  {
    "objectID": "src/docs/content/process_names.html#application",
    "href": "src/docs/content/process_names.html#application",
    "title": "Data Processing Names",
    "section": "Application",
    "text": "Application\n\n# load data \ndir = file.path(\"data\", \"processed\")\nfile = list.files(dir, pattern = \"scholarid.Rds\", full.names = TRUE)\n\ndata = freadRDS(file[length(file)]) \n\nnames = data |&gt;\n  select(naam) |&gt;\n  arrange(naam) |&gt;\n  distinct() |&gt;\n  drop_na() |&gt;\n  parse_names() |&gt;\n  extract_initials() |&gt;\n  patch_names() |&gt;\n  extract_maiden_name() |&gt;\n  format_names() |&gt;\n  add_uid()\n\nJoining with `by = join_by(clean_name)`\n\nfsaveRDS(\n  names, \n  'names'\n)\n\n[1] \"SAVING: ./data/processed/20251027names.Rds\""
  },
  {
    "objectID": "src/docs/content/process_ethnicity.html",
    "href": "src/docs/content/process_ethnicity.html",
    "title": "Data Processing Ethnicity",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  605486 32.4    1372058 73.3         NA   715802 38.3\nVcells 1121585  8.6    8388608 64.0      16384  2012236 15.4\n\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(httr2)\nlibrary(rvest)\nlibrary(xml2)\nlibrary(purrr)\nlibrary(RCurl)\nlibrary(fuzzyjoin)\nlibrary(stringi)\nlibrary(DemografixeR)\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()"
  },
  {
    "objectID": "src/docs/content/process_ethnicity.html#getting-started",
    "href": "src/docs/content/process_ethnicity.html#getting-started",
    "title": "Data Processing Ethnicity",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  605486 32.4    1372058 73.3         NA   715802 38.3\nVcells 1121585  8.6    8388608 64.0      16384  2012236 15.4\n\n\n\n# load and activate packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(httr2)\nlibrary(rvest)\nlibrary(xml2)\nlibrary(purrr)\nlibrary(RCurl)\nlibrary(fuzzyjoin)\nlibrary(stringi)\nlibrary(DemografixeR)\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n.load_quarto_dependencies()"
  },
  {
    "objectID": "src/docs/content/process_ethnicity.html#functions",
    "href": "src/docs/content/process_ethnicity.html#functions",
    "title": "Data Processing Ethnicity",
    "section": "Functions",
    "text": "Functions\n\nScraper Configuration\nThis function builds a resilient GET request to a target page and returns a uniform result object instead of throwing on transport errors.\n\nSuccess check. is_ok() flags responses with HTTP status in the 200–299 range.\nBrowser-like request. request_last_name() constructs an httr2 request with a Mac Chrome user agent, Dutch/English Accept-Language, and broad Accept headers to mimic a real browser and reduce blocking.\nConnection settings. It sets a 30-second timeout and (deliberately) disables SSL peer verification to cope with misconfigured certificates on legacy servers.\nRetry policy. On transient errors (HTTP 429 or 5xx), it retries up to 4 times with jittered exponential backoff (runif(0.5–1.2) × 2^(try−1)), which spreads load and avoids thundering herds.\nPoliteness delay. If a global pause is set, it sleeps pause + U(0, 0.4) seconds before firing the request to throttle scraping.\nError handling. The actual HTTP call is wrapped in tryCatch(). Instead of stopping, it returns a structured list:\n\nok (logical) – success per is_ok()\nstatus (integer) – HTTP status or NA on transport error\nurl (character) – the requested URL\nresp (httr2 response) – raw response on success, NULL on error\nerror (condition) – the caught error on failure\n\n\nThis design lets downstream code branch cleanly on res$ok and inspect res$status or res$error without breaking the pipeline.\n\nis_ok = function(resp) resp_status(resp) &gt;= 200 && resp_status(resp) &lt; 300\n\nrequest_last_name = function(base_url, pause=0.5){\n  # configure user agent\n  ua = paste(\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 15_5)\",\n    \"AppleWebKit/537.36 (KHTML, like Gecko)\",\n    \"Chrome/129.0.0.0 Safari/537.36\"\n  )\n\n  req = request(base_url) |&gt;\n    req_user_agent(ua) |&gt;\n    # disable SSL verification\n    req_options(ssl_verifypeer = 0) |&gt;\n    req_timeout(30) |&gt;\n    req_headers(\n      \"Accept\" = paste0(\"text/html,application/xhtml+xml,\",\n                        \"application/xml;q=0.9,*/*;q=0.8\"),\n      \"Accept-Language\" = \"nl,en;q=0.8\"\n    ) |&gt;\n    req_retry(\n      max_tries = 4,\n      backoff = ~ runif(1, 0.5, 1.2) * (2 ^ (.x - 1)),  # jittered exponential\n      is_transient = function(resp) {\n        code &lt;- resp_status(resp)\n        isTRUE(code == 429L || (code &gt;= 500L & code &lt; 600L))\n      }\n    )\n\n  # polite pause + jitter\n  if (pause &gt; 0) Sys.sleep(pause + runif(1, 0, 0.4))\n\n  # CRITICAL: don't throw on transport errors\n  resp &lt;- tryCatch(\n    req_perform(req),\n    error = function(e) {\n      attr(e, \"nvb_url\") &lt;- url\n      e\n    }\n  )\n\n  # Return a uniform list the caller can inspect\n  if (inherits(resp, \"error\")) {\n    res = list(\n      ok = FALSE,\n      status = NA_integer_,\n      url = url,\n      resp = NULL,\n      error = resp\n    )\n  } else {\n    res = list(\n      ok = is_ok(resp),\n      status = resp_status(resp),\n      url = url,\n      resp = resp,\n      error = NULL\n    )\n  }\n  return(res)\n}\n\n\n\nConfigure URL and Extracters\n\nformat_url() – Build a query URL for CBG Familienamen\nTakes a name and constructs a browser-ready URL for the CBG surnames site, encoding spaces as + and adding query parameters for multiple name fields. Optional .what = \"info\" appends the path to the analysis/etymology page. Returns the full URL string.\n\nformat_url = function(\n    name,\n    base = \"https://www.cbgfamilienamen.nl/nfb/detail_naam.php?\",\n    .what = \"base\"\n  ){\n  # format_name for URL\n  formatted_name = name |&gt; \n    URLencode(reserved = TRUE) |&gt;\n    str_replace_all(pattern = \"%20\", '+')\n\n  # configure url\n  url = paste0(\n    base,\n    \"gba_naam=\", formatted_name,\n    \"&gba_lcnaam=\", tolower(formatted_name),\n    \"&nfd_naam=\", formatted_name\n  )\n\n  if (.what == 'info'){\n    url = paste0(url, \"&info=analyse+en+verklaring\")\n  }\n  \n  return(url)\n}\n\n\n\nextract_info() – Parse analysis/etymology text from response\nGiven a response wrapper r (with r$resp from httr2), reads the HTML body and extracts the “kenmerken/verklaring” section. It splits the page text, trims boilerplate until the © footer, and returns a single “;”-separated string with the extracted info (or “” if not found).\n\nextract_info = function(r){\n  html = read_html(resp_body_string(r$resp))\n  text = html |&gt;\n    html_element(\"body\") |&gt;\n    html_text()\n\n  info = c(\"\")\n  if (str_detect(text, regex('kenmerken:|verklaring:'))){\n    parts = str_split_fixed(text, pattern=regex('kenmerken:|verklaring:'), 2)[2] |&gt;\n      str_split(pattern = regex(\"[\\\\n|\\\\t|\\\\s]{3,20}\")) |&gt;\n      unlist()\n\n    idx = which(grepl(regex(\"©\"), parts))\n\n    if (length(idx) &gt;= 1) {\n      info = parts[seq_len(idx[1] - 1)]\n      info = as.vector(info[nzchar(info)])\n    }\n  }\n\n  # drop empty strings\n  # info = info[nzchar(info)]\n  return(paste(unname(info), collapse = \"; \"))\n}\n\n\n\nextract_count() – Retrieve occurrence count from tables\nParses all HTML tables from r$resp and searches for the table layout containing the national count. If found, pulls the value at row 2, column 2 (per the site’s structure) and returns it as an integer; throws an error if no tables are present and returns NA when the expected layout is missing.\n\nextract_count = function(r){\n  # extract all tables on the page\n  html = read_html(resp_body_string(r$resp))\n  tables = html_table(html, header=FALSE)\n\n  # select the first table if a table if found\n  if (length(tables) == 0) stop(\"No tables were found\")\n  \n  # set count value\n  count = NA_integer_\n  if (length(tables) &gt;= 4) {\n    for (tab in tables){\n      i = nrow(tab)\n      if (i &gt;= 5){\n        count = tab[2,2] |&gt; pull()\n      }\n    }\n  }\n\n  return(count)\n}\n\n\n\n\nTidy Scrape Wrappers\nThese utilities wrap the earlier extractors and assemble a tidy result for a single surname. - safe_count() / safe_info() wrap extract_count() and extract_info() in tryCatch(), returning a default (NA_integer_ / NA_character_) on any error or warning. This guarantees downstream code receives a value even when pages are missing or malformed. - get_name_row() takes a name and performs up to two requests: 1. Builds the base URL with format_url(name) and fetches it via request_last_name(). If the HTTP result is ok, it parses the national occurrence count with safe_count(); otherwise it records NA. 2. Only if a non-missing count was obtained, it builds the “info” URL (.what = \"info\") and requests it, then extracts the analysis/etymology info with safe_info() (again only when both requests succeeded).\nBoth count and info are coerced to character for consistency. The function returns a one-row tibble with last_name, name_count, and info, providing a compact, fault-tolerant record for each queried surname.\n\nsafe_count = function(r, default = NA_integer_) {\n  tryCatch(extract_count(r),\n           error = function(e) default,\n           warning = function(w) default)\n}\n\nsafe_info = function(r, default = NA_character_) {\n  tryCatch(extract_info(r),\n           error = function(e) default,\n           warning = function(w) default)\n}\n\nget_name_row = function(name, count=NA_character_, info=NA_character_){\n  # scrape the count information\n  r1 = format_url(name) |&gt;\n    request_last_name() \n\n  count = if (isTRUE(r1$ok)) safe_count(r1) else NA_character_\n  count = as.character(count)\n\n  # scrape info if the first scrape yielded success\n  if (!is.na(count)){\n  r2 = format_url(name, .what=\"info\") |&gt;\n    request_last_name()\n\n  info = if (isTRUE(r2$ok) && isTRUE(r1$ok)) safe_info(r2) else NA_character_\n  info = as.character(info)\n  }\n\n\n  as_tibble(list(\n    last_name = name, \n    name_count = count,\n    info = info\n  ))\n}\n\n\nadd_origin5 = function(ethnicity){\n  ethnicity_patch = readxl::read_excel(\n      file.path('data', 'utils', 'origin_patch.xlsx')\n    ) |&gt;\n    mutate(last_name_norm = normalize_name(last_name))\n\n  ethnicity |&gt;\n    mutate(last_name_norm = normalize_name(last_name)) |&gt;\n    fuzzyjoin::stringdist_left_join(\n      ethnicity_patch,\n      by = \"last_name_norm\",\n      max_dist = 0.5\n    ) |&gt;\n    rename(\n      \"last_name\" = \"last_name.x\",\n      \"origin5\" = \"origin\"\n    ) |&gt;\n    select(\n      -last_name_norm.x, -count, \n      -last_name_norm.y, -last_name.y,\n    )\n}\n\nadd_origin = function(data) {\n  data |&gt;\n    mutate(\n      origin1 = str_extract_all(info, regex(\"[:upper:]([:lower:]{2,}) naam\")),\n      origin2 = ifelse(\n        str_detect(info, \"afkomstig uit\"),\n        str_remove(info, \".*afkomstig uit\"),\n        NA_character_\n      ),\n      origin3 = str_extract(\n        info, \"[:upper:]([:lower:]{2,}) (achter)?(familie)?(beroeps)?naam\"\n      )\n    ) |&gt;\n    # clean origin information\n    mutate(\n      # Exclude Jewish people form Origin information\n      origin1 = str_remove(origin1, \"Joodse naam\"),\n      # Only s\n      origin2 = str_remove(origin2, \"\\\\..*\") |&gt;\n        str_remove(\"\\\\;.*\") |&gt;\n        str_remove(\"\\\\(.*\"),\n      regional = str_detect(\n        origin2, paste0(\n          \"(dorp)|(plaats)|(gemeente)\",\n          \"|(graafschap)|(stad)|(deel)|(Friesland)\"\n        )\n      ),\n      has_particle = str_detect(\n        last_name, regex(\"^(van |de )\")\n      ),\n      origin2 = ifelse(isTRUE(regional), NA_character_, origin2),\n      origin3 = origin3 |&gt;\n        str_remove(\"D(i)?e(ze)? (familie)?(achter)?(beroeps)?naam\") |&gt;\n        str_remove(\"Een (familie)?(achter)?(beroeps)?naam\") |&gt;\n        str_remove(\"Zijn (familie)?(achter)?(beroeps)?naam\") |&gt;\n        str_remove(\"Als (familie)?(achter)?(beroeps)?naam\") |&gt;\n        str_remove(\"Joodse (familie)?(achter)?naam\") |&gt;\n        str_remove(\"Bijbelse (familie)?(achter)?naam\"),\n      origin4 = str_detect(info, \"andere taal\")\n    ) |&gt;\n    select(-regional)\n}\n\n\nclean_ethnicity = function(data){\n  data |&gt;\n    mutate(\n      origin1 = ifelse(origin1 == 'character(0)', NA_character_, origin1),\n      origin = case_when(\n        length(origin1) &gt; 1 ~ origin3,\n        .default = origin1\n      ),\n      origin = coalesce(origin, origin5),\n      # origin = case_when(\n      #   is.na(origin) & !is.na(count) & !is.na(info) ~ \"Nederlandse naam\",\n      #   has_particle ~ \"Nederlandse naam\",\n      #   .default = origin\n      # ),\n      origin = str_remove(origin, regex(\"\\\\ .*\")),\n      origin = case_when(\n        str_detect(origin, \"Christelijk\") ~ \"Nederlandse\",\n        str_detect(origin, \"Friese\") ~ \"Nederlandse\",\n        str_detect(origin, \"Tilburgse\") ~ \"Nederlandse\",\n        origin ==  \"\" ~ NA_character_,\n        str_detect(origin, \"Waalse\") ~ \"Belgische\",\n        str_detect(origin, \"Catalaanse\") ~ \"Spaanse\", \n        .default = origin\n      )\n    ) |&gt;\n    select(last_name, origin, name_count)\n}\n\n\nlibrary('DemografixeR')\n\nnationalize_name = function(idx) {\n  origin_cache = readRDS(file.path('data', 'utils', \"origin_cache.Rds\"))\n\n  # load api key from secrets file\n  dotenv::load_dot_env()\n  APIKEY &lt;- Sys.getenv(\"GENDERIZE_API_KEY\")\n\n  # select uncached names\n  idx =  idx |&gt; filter(!search_name %in% origin_cache$name)\n\n  # select last_names\n  last_names = idx$search_name |&gt; na.omit() |&gt; unique() |&gt; sort()\n  # last_names = last_names[12:22]\n\n  # fetch nationality results\n  hold = c()\n  for (name in last_names){\n    resp = nationalize(name, sliced=FALSE, apikey = APIKEY, simplify = FALSE)\n    hold[[name]] = resp \n  }\n\n  # combine cache with results and put new cache results\n  origin = bind_rows(origin_cache, bind_rows(hold)) |&gt; \n    distinct(.keep_all = TRUE)\n\n  if (\"country\" %in% colnames(origin)) origin = origin |&gt; select(-country)\n\n  saveRDS(origin, file.path('data', 'utils', \"origin_cache.Rds\"))\n\n  return(origin)\n}\n\n\n# o = res |&gt;\n#   mutate(too_uncertain = probability &lt; 0.05)\n\nselect_best_match = function(res){\n  \n\n  res |&gt; \n    rename(\"last_name\" = \"name\") |&gt;\n    # get the best match\n    group_by(last_name) |&gt;\n    slice(1) \n    # |&gt;\n    # add country label for inspection\n    # left_join(\n    #   iso |&gt; select(alpha_2, country_name),\n    #   by=join_by(country_id==alpha_2)\n    # )\n}\n\n\nname_to_origin = function(idx) {\n  ethnicity_cache = readRDS(file.path('data', 'utils', 'cbg_cache.Rds'))\n\n  # select uncached names\n  idx =  idx |&gt; filter(!search_name %in% ethnicity_cache$last_name)\n\n  # select last_names\n  last_names = idx$search_name |&gt; na.omit() |&gt; unique() |&gt; sort()\n\n  hold = c()\n  for (name in last_names){\n    if (!name %in% ethnicity_cache$last_name){\n      hold[[name]] = get_name_row(name)\n    }\n  }\n\n  res = bind_rows(ethnicity_cache, bind_rows(hold))\n  saveRDS(res,  file.path('data', 'utils', 'cbg_cache.Rds'))\n\n  return(res)\n}\n\n\nharmonize_ethnicity = function(origin) {\n  iso = readxl::read_xlsx(file.path('data', 'utils', 'iso3611_codes.xlsx'))\n\n  origin = origin |&gt;\n    mutate(\n      name_count = name_count |&gt; str_replace('&lt; 5', '5') |&gt; as.integer(),\n      name_count = ifelse(is.na(name_count), 0, name_count),\n      ethnicity = case_when(\n        str_detect(origin, 'Nederlandse') & (name_count &gt; 200) ~ \"NL\",\n        str_detect(origin, 'Chinese') & (name_count &gt; 200) ~ \"CN\",\n        str_detect(origin, 'Duitse') & (name_count &gt; 200) ~ \"DE\",\n        str_detect(origin, 'Marokkaanse') & (name_count &gt; 200) ~ \"MA\",\n        str_detect(origin, 'Turkse') & (name_count &gt; 200) ~ \"TR\",\n        .default = country_id\n      ),\n      ethnicity = case_when(\n        str_detect(search_name, 'Wachter') ~ \"NL\",\n        str_detect(search_name, 'Das') ~ \"NL\",\n        str_detect(search_name, \"Metinsoy\") ~ \"TR\",\n        str_detect(search_name, \"Wagner\") ~ \"TR\",\n        str_detect(search_name, \"Mos\") ~ \"NL\",\n        str_detect(search_name, \"Vlieg\") ~ \"NL\",\n        str_detect(search_name, \"Knigge\") ~ \"NL\",\n        str_detect(search_name, \"Bol\") ~ \"NL\",\n        str_detect(search_name, \"Deen\") ~ \"NL\",\n        str_detect(search_name, \"Vrooman\") ~ \"NL\",\n        str_detect(search_name, \"Verdun\") ~ \"NL\",\n        str_detect(search_name, \"Merlo\") ~ \"NL\",\n        str_detect(search_name, \"Phillips\") ~ \"BR\",\n\n        .default = ethnicity\n      ),\n      count = case_when(\n        country_id != ethnicity ~ NA,\n        .default = count\n      ),\n      probability = case_when(\n        country_id != ethnicity ~ NA,\n        .default = probability\n      )\n    ) |&gt;\n    select(uid, search_name, ethnicity, count, probability) |&gt;\n    distinct(uid, search_name, ethnicity, .keep_all=TRUE) |&gt;\n    left_join(\n      iso |&gt; select(alpha_2, country_name),\n      by = join_by(ethnicity == alpha_2)\n    )\n}"
  },
  {
    "objectID": "src/docs/content/process_ethnicity.html#application",
    "href": "src/docs/content/process_ethnicity.html#application",
    "title": "Data Processing Ethnicity",
    "section": "Application",
    "text": "Application\n\ndir = file.path('data', 'processed')\nfile = list.files(dir, pattern = 'names.Rds')\nnames = readRDS(file.path(dir, file[length(file)]))\n\n# create an name index\nidx = names |&gt;\n  distinct(particle, last_name, .keep_all = TRUE) |&gt;\n  unite(search_name, particle:last_name, sep=\" \", na.rm=TRUE, remove = FALSE) |&gt;\n  unite(search_name_full, particle:maiden_name, sep=\" \", na.rm=TRUE, remove = FALSE) \n\n# scrape using nationalizer\nres = nationalize_name(idx) |&gt;\n  select_best_match()\n\n# scrape using family name database\nres2 = name_to_origin(idx)|&gt; \n  add_origin() |&gt;\n  add_origin5() |&gt;\n  clean_ethnicity()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `origin1 = str_remove(origin1, \"Joodse naam\")`.\nCaused by warning in `stri_replace_first_regex()`:\n! argument is not an atomic vector; coercing\n\n# combine scraped information\norigin = idx |&gt;\n  left_join(res, by=join_by(search_name==last_name)) |&gt;\n  left_join(res2, by=join_by(search_name==last_name)) |&gt;\n  select(uid, last_name, search_name, type:name_count) |&gt;\n  harmonize_ethnicity()\n\nfsaveRDS(origin, 'ethnicity')\n\n[1] \"SAVING: ./data/processed/20251027ethnicity.Rds\""
  }
]