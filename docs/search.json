[
  {
    "objectID": "src/docs/content/process_gender.html",
    "href": "src/docs/content/process_gender.html",
    "title": "Data Processing Gender",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  605157 32.4    1371120 73.3         NA   715800 38.3\nVcells 1116551  8.6    8388608 64.0      16384  2012047 15.4\n\n\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n\n# load and activate packages\nfpackage.check(c(\n  'tidyverse', 'readxl',  'stringr', \n  'lubridate', 'httr2', 'rvest', 'xml2',\n  \"purrr\"\n))"
  },
  {
    "objectID": "src/docs/content/process_gender.html#getting-started",
    "href": "src/docs/content/process_gender.html#getting-started",
    "title": "Data Processing Gender",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  605157 32.4    1371120 73.3         NA   715800 38.3\nVcells 1116551  8.6    8388608 64.0      16384  2012047 15.4\n\n\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n\n# load and activate packages\nfpackage.check(c(\n  'tidyverse', 'readxl',  'stringr', \n  'lubridate', 'httr2', 'rvest', 'xml2',\n  \"purrr\"\n))"
  },
  {
    "objectID": "src/docs/content/process_gender.html#functions",
    "href": "src/docs/content/process_gender.html#functions",
    "title": "Data Processing Gender",
    "section": "Functions",
    "text": "Functions\n\nCall Meertens Voornamen Databank\n\nis_ok = function(resp) resp_status(resp) &gt;= 200 && resp_status(resp) &lt; 300\n\nrequest_gender = function(\n    first_name,\n    base = \"https://nvb.meertens.knaw.nl/naam/is/\",\n    pause = 0.5\n  ){\n  # configure url for scraping\n  url  = paste0(base, URLencode(tolower(first_name), reserved = TRUE))\n\n  # configure user agent\n  ua = paste(\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 15_5)\",\n    \"AppleWebKit/537.36 (KHTML, like Gecko)\",\n    \"Chrome/129.0.0.0 Safari/537.36\"\n  )\n\n  req = request(url) |&gt;\n    req_user_agent(ua) |&gt;\n    req_timeout(30) |&gt;\n    req_headers(\n      \"Accept\" = \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n      \"Accept-Language\" = \"nl,en;q=0.8\"\n    ) |&gt;\n    # Retry on 429/5xx, and *also* on network hiccups:\n    req_retry(\n      max_tries = 4,\n      backoff = ~ runif(1, 0.5, 1.2) * (2 ^ (.x - 1)),  # jittered exponential\n      is_transient = function(resp) {\n        code &lt;- resp_status(resp)\n        isTRUE(code == 429L || (code &gt;= 500L & code &lt; 600L))\n      }\n    )\n\n  # polite pause + jitter\n  if (pause &gt; 0) Sys.sleep(pause + runif(1, 0, 0.4))\n\n  # CRITICAL: don't throw on transport errors\n  resp &lt;- tryCatch(\n    req_perform(req),\n    error = function(e) {\n      attr(e, \"nvb_url\") &lt;- url\n      e\n    }\n  )\n\n  # Return a uniform list the caller can inspect\n  if (inherits(resp, \"error\")) {\n    return(list(ok = FALSE, status = NA_integer_, url = url, resp = NULL, error = resp))\n  }\n\n  list(ok = is_ok(resp), status = resp_status(resp), url = url, resp = resp, error = NULL)\n}\n\n\n\nExtract Gender Information\n\nextract_gender_information = function(\n    resp,\n    first_name\n  ){\n  # extract all the tables on the page\n  html = read_html(resp_body_string(resp))\n  tables = html_table(html, header=TRUE)\n\n  # select the first table if a table if found\n  if (length(tables) == 0) stop(\"No tables were found\")\n  tab = tables[[1]]\n\n  # extract information from table\n  male_count = tab[1, 3] |&gt; pull()\n  male_count = ifelse(male_count == '--', 0, as.numeric(male_count))\n  female_count = tab[5, 3] |&gt; pull() \n  female_count = ifelse(female_count == '--', 0, as.numeric(female_count))\n  probability_male = male_count / (female_count + male_count) \n\n  # configure results table\n  res = tibble::tribble(\n    ~first_name, ~male_count, ~female_count, ~probability_male,\n    first_name, male_count,  female_count,  probability_male\n  )\n  return(res)\n}\n\n\n\nHelper functions\n\nsafe_extract = purrr::possibly(\n  extract_gender_information,\n  otherwise = tibble::tibble(\n    first_name       = NA_character_,\n    male_count       = NA_integer_,\n    female_count     = NA_integer_,\n    probability_male = NA_real_\n  )\n)\n\nget_gender_row = function(name, gender) {\n  # If cached, return from cache\n  if (name %in% gender$first_name) {\n    return(gender |&gt; filter(first_name == name))\n  }\n\n  r &lt;- request_gender(name)\n\n  # If transport error or HTTP not OK, surface status & keep going\n  if (!isTRUE(r$ok)) {\n    return(tibble(\n      first_name       = NA_character_,\n      male_count       = NA_integer_,\n      female_count     = NA_integer_,\n      probability_male = NA_real_\n    ))\n  }\n\n  out &lt;- safe_extract(r$resp, name) |&gt;\n    mutate(first_name = name)\n\n  out\n}\n\n\n\nPatch Difficult Names\n\npatch_gender_on_splits = function(gender){\n  # set gender cache\n  gender_cache = gender |&gt; drop_na()\n  # select authors without gender, and split their names\n  selection = gender |&gt;\n    filter(is.na(male_count)) |&gt;\n    mutate(first_name_split = str_split(first_name, ' ')) |&gt;\n    unnest_longer(first_name_split) |&gt;\n    select(first_name, first_name_split)\n\n  # get first names from selection\n  first_names = selection |&gt;\n    select(first_name_split) |&gt;\n    pull() |&gt;\n    unique()\n\n  # patch gender --------------------------------------\n  gender_patch = purrr::map_dfr(\n    first_names, get_gender_row, gender = gender_cache\n  )\n\n  # aggregate gender information\n  gender_patch = selection |&gt;\n    left_join(\n      gender_patch, \n      by=join_by(first_name_split == first_name)\n    ) |&gt;\n    drop_na() |&gt;\n    # take the average gender count and probablity\n    # for names where both splits yielded a gender\n    # result\n    group_by(first_name) |&gt;\n    summarise(\n      male_count = as.integer(mean(male_count)),\n      female_count = as.integer(mean(female_count)),\n      probability_male = mean(probability_male)\n    ) |&gt;\n    ungroup() \n\n  gender |&gt; rows_update(gender_patch)\n}\n\n\n\nClean Gender Information\n\nclean_gender = function(data){\n  gender |&gt;\n  mutate(\n    gender = ifelse(\n      probability_male &gt;= 0.5,\n      'male','female'\n    ),\n    count = ifelse(\n      gender == 'male',\n      male_count, female_count\n    ),\n    prob = ifelse(\n      gender == 'male',\n      probability_male, 1 - probability_male\n    )\n  ) |&gt;\n  select(first_name, gender, prob, count)\n}\n\n\n\nPatch Missing Gender\n\npatch_missing_gender = function(data){\n  female_names = c(\n    \"Alaxandra\",   \"Alinson\",     \"Avyanthi\",\n    \"Brunilda\",    \"Busisiwe\",    \"Diliara\",      \n    \"Dolivė\",      \"Echo\",        \"Guangyu\",\n    # mistake in name, has been patched with _create_name_corrections\n    \"Guangye\", \n    \"Gul-i-Hina\",  \"Haebin\",      \"Haisu\",\n    # Phoebe Kisibi Mbasalaki was incorrectly coded, has been patched with _create_name_corrections \n    \"Kisubi\",      \"Pheobe\",\n    \"Liubov\",      \"Madalina\",    \"Majolijn\",    \n    \"Mansoureh\",   \"Nankyung\",    \"Nilmawati\",   \n    \"Nodira\",      \"Noyonika\",    \"Radostina\",   \n    \"Rojika\",      \"Rozenmarijn\", \"Sayoni\",\n    \"Seonoki\",     \"Shelliann\",   \"Shiming\",     \n    \"Siggie\",      \"Siztine\",     \"Sungmi\",      \n    \"Talinta\",     \"Teana\",       \"Xingna\",\n    \"Yuliia\",      \"Zhiyi\"\n  )\n\n  male_names = c(\n    \"Alborno\",     \"Chenchen\",    \"Chendi\",\n    \"Chunglin\",    \"Chuyu\",       \"Diliara\",\n    \"Gjovalin\",    \"Kirils\",      \"Kyohee\",\n    \"Madhud\",      \"Quichen\",     \"Soeren\",\n    \"Teana\",       \"Tanzhe\",     \"Vishwesh\",\n    \"Weverthon\"\n  )\n\n  data |&gt;\n    mutate(\n      gender = case_when(\n        !is.na(gender) ~ gender,\n        first_name %in% female_names ~ 'female',\n        first_name %in% male_names ~ 'male',\n        .default = gender\n      )\n    )\n}"
  },
  {
    "objectID": "src/docs/content/process_gender.html#application",
    "href": "src/docs/content/process_gender.html#application",
    "title": "Data Processing Gender",
    "section": "Application",
    "text": "Application\n\ndir = file.path('data', 'processed')\nfile = list.files(dir, pattern = 'names.Rds')[[1]]\nnames = readRDS(file.path(dir, file))\n\nfirst_names = names |&gt;\n  distinct(first_name) |&gt;\n  filter(!is.na(first_name), first_name != \"\") |&gt;\n  pull(first_name)\n\ngender_cache = readRDS(file.path(\"data\", \"utils\", \"nvb_gender.Rds\")) |&gt; \n  # select(-status) |&gt;\n  tidyr::drop_na()\n\ngender = purrr::map_dfr(\n    first_names, \n    get_gender_row, \n    gender = gender_cache\n  ) |&gt;\n  patch_gender_on_splits() \n\nMatching, by = \"first_name\"\n\n  # |&gt;\n  # select(-status)\n\nsaveRDS(\n  gender |&gt; drop_na(),\n  file.path('data', 'utils', \"nvb_gender.Rds\")\n)\n\ngender |&gt; \n  clean_gender() |&gt;\n  patch_missing_gender() |&gt;\n  fsaveRDS('gender')\n\n[1] \"SAVING: ./data/processed/20251014gender.Rds\""
  },
  {
    "objectID": "src/docs/content/scrape_authorid.html",
    "href": "src/docs/content/scrape_authorid.html",
    "title": "Data Preparation OpenAlex-ID",
    "section": "",
    "text": "Tip\n\n\n\nrough first working code for the collection of scholar_ids using openalexr\nI optimized the algorithm to not just select the top case, but allows for multiple ids (in rows) per person, in the decision rules i use: - semantic similarity - matches of university_id and institution_id"
  },
  {
    "objectID": "src/docs/content/scrape_authorid.html#getting-started",
    "href": "src/docs/content/scrape_authorid.html#getting-started",
    "title": "Data Preparation OpenAlex-ID",
    "section": "Getting Started",
    "text": "Getting Started\n\n# clear the global environment\nrm(list = ls())\ngc()\n\n\nsource(\"src/utils/custom_functions.r\")\n\n# load and activate packages\nfpackage.check(c(\n  'tidyverse', 'readxl',  'stringr', \n  'lubridate', 'openalexR', 'rvest', 'jsonlite',\n  'cli'\n))\n\noptions(openalexR.mailto = \"jos.slabbekoorn@ru.nl\")\n\n\ndir = file.path('data', 'processed')\nfile = list.files(dir, pattern = 'names.Rds')\nnames = freadRDS(file.path(dir, file))\n\n\nfile = list.files(dir, pattern = 'scholarid.Rds')\nscholar = freadRDS(file.path(dir, file)) |&gt;\n  select(naam, university) |&gt;\n  distinct(.keep_all = TRUE) |&gt;\n  unnest_longer(university) \n  # |&gt;\n  # left_join(names) |&gt;\n  # unite(\n  #   initials:last_name, \n  #   col=name, sep = ' ', \n  #   na.rm=TRUE, remove=FALSE) |&gt;\n  # unite(\n  #   c(name, maiden_name), \n  #   col=name2, sep = '-', \n  #   na.rm=TRUE, remove=FALSE)\n\n\nscholar = scholar |&gt;\n  mutate(\n    university = university |&gt;\n      str_replace(\"(Uni |uni)\", \"University \"),\n    university_name = case_when(\n      str_detect(university, \"VU\") ~ \"Free University Amsterdam\",\n      str_detect(university, \"(UvA|Uva)\") ~ \"University of Amsterdam\",\n      str_detect(university, \"Leiden\") ~ \"Leiden University\",\n      university == 'RU' ~ \"Radboud University Nijmegen\",\n      str_detect(university, \"(UU|UCU)\") ~ \"Utrecht University\",\n      university == 'EUR' ~ \"Erasmus University Rotterdam\",\n      str_detect(university, '(RUG|UvG)') ~ \"University of Groningen\",\n      str_detect(university, '(UvT|Uvt|Tilburg)') ~ \"University of Tilburg\",\n      str_detect(university, 'WUR') ~ \"Wageningen University & Research\",\n      university == 'Uni Gothenburg' ~ \"University of Gothenburg\",\n      university == 'TU Delft' ~ \"Technical University Delft\",\n      str_detect(university, 'Milano') ~ 'University of Milan',\n      # fix for mistake in affilation for Sabine Mokry,\n      str_detect(university, 'Berlijn') ~ 'University of Hamburg',\n      str_detect(university, 'Trento') ~ 'University of Trento',\n      str_detect(university, 'Stockholm') ~ 'Stockholm University',\n      str_detect(university, 'Gent') ~ \"Universiteit Gent\",\n      university == \"Politie\" ~ NA_character_,\n      university == \"UvH\" ~ NA_character_,\n      .default = university\n    ),\n    university_name2 = case_when(\n      str_detect(university, \"VU\") ~ \"Vrije Universiteit Amsterdam\",\n      str_detect(university, \"(UvA|Uva)\") ~ \"Universiteit van Amsterdam\",\n      str_detect(university, \"Leiden\") ~ \"Leiden Universiteit\",\n      university == 'RU' ~ \"Radboud Universiteit Nijmegen\",\n      str_detect(university, \"(UU|UCU)\") ~ \"Universiteit Utrecht\",\n      university == 'EUR' ~ \"Erasmus Universiteit Rotterdam\",\n      str_detect(university, '(RUG|UvG)') ~ \"Rijksuniversiteit Groningen\",\n      str_detect(university, '(UvT|Uvt|Tilburg)') ~ \"Universiteit van Tilburg\",\n      str_detect(university, 'WUR') ~ \"Wageningen University & Research\",\n      university == 'Uni Gothenburg' ~ \"Universiteit van Gothenburg\",\n      university == 'TU Delft' ~ \"Technische Universiteit Delft\",\n      str_detect(university, 'Milano') ~ 'Università degli Studi di Milano Statale',\n      # fix for mistake in affilation for Sabine Mokry,\n      str_detect(university, 'Berlijn') ~ 'Universität Hamburg',\n      str_detect(university, 'Trento') ~ 'Università degli Studi di Trento',\n      str_detect(university, 'Cologne') ~ 'Universität zu Köln',\n      str_detect(university, 'Stockholm') ~ 'Stockholms Universitet',\n      str_detect(university, 'Bocconi') ~ \"Università Bocconi\",\n      str_detect(university, 'Koc') ~ \"Koç Üniversitesi\",\n      str_detect(university, 'Gent') ~ \"Ghent University\",\n      str_detect(university, 'Gothenburg') ~ \"Göteborgs universitet\",\n      str_detect(university, \"Turku\") ~ \"Turun Yliopisto\",\n      str_detect(university, 'Lausane') ~ \"Université de Lausanne\",\n      str_detect(university, \"Leipzig\") ~ \"Universität Leipzig\",\n      str_detect(university, \"Linköping\") ~ \"Linköpings universitet\",\n      .default = NA_character_\n    )\n  ) |&gt;\n  drop_na() |&gt;\n  pivot_longer(\n    cols = university_name:university_name2, \n    names_to = \"var\",\n    values_to = \"university_name\"\n  ) |&gt;\n  select(-var)\n\n\ninstitutions = scholar$university_name |&gt; unique()\n\n\n# scholar = scholar |&gt;\n#   mutate(\n#     university_name = university |&gt;\n#       str_replace(\"(Uni |uni)\", \"University \") |&gt;\n#       str_replace('(UU)', 'Utrecht University') |&gt;\n#       str_replace('(UCU)', 'Utrecht University') |&gt;\n#       str_replace('(Leiden|Leiden University)', 'Leiden University') |&gt;\n#       str_replace('(RUG)', 'University of Groningen') |&gt;\n#       str_replace('(RU)', 'Radboud University Nijmegen') |&gt;\n#       str_replace('(UvA|Uva)', 'University of Amsterdam') |&gt;\n#       str_replace('(VU)', 'Free University Amsterdam') |&gt;\n#       str_replace('(EUR)', 'Erasmus University Rotterdam') |&gt;\n#       str_replace('(UvT|Tilburg|Uvt)', 'University of Tilburg') |&gt;\n#       str_replace('(TU Delft)', 'Technical University Delft') |&gt;\n#       str_replace('(WUR)', 'Wageningen University & Research') |&gt;\n#       str_replace('(Universita degli studi di Milano)', 'University of Milan') |&gt;\n#       str_replace('(Politie)', NA_character_) |&gt;\n#       str_replace('(UvH)', NA_character_) |&gt;\n#       str_replace('(UvG)', \"University of Groningen\") |&gt;\n#       # fixing mistake for Sabine Mokry\n#       str_replace('University Berlijn', \"University of Hamburg\") |&gt;\n#       str_replace('Trento University', \"University of Trento\") |&gt;\n      \n#   ) |&gt;\n#   filter(!is.na(university_name))\n\n# # institutions = c(\n# #   scholar$university_name |&gt; unique(),\n# # )\n\n# institutions = scholar$university_name |&gt; unique()\n\n\noa_fetch_institution = function(institution, pause=0){\n  if (pause &gt; 0) Sys.sleep(pause)\n  oa_fetch(\n    entity = \"institutions\",\n    search=institution,\n    mailto = \"jos.slabbekoorn@ru.nl\"\n  )$id[1]\n}\n\nhold = c()\nfor (institution in institutions){\n  id = oa_fetch_institution(institution)\n  hold[[institution]] = tibble::tibble(\n    university_name = institution,\n    university_url = id, \n    university_id = str_remove(id, 'https://openalex.org/')\n  )\n}\n\ninstitutions = bind_rows(hold) |&gt;\n  drop_na()\nscholar = scholar |&gt;\n  left_join(institutions) |&gt;\n  distinct(naam, university_id, .keep_all = TRUE)\n\n\n# library(dplyr)\nlibrary(stringdist)\nlibrary(stringi)\n\nnormalize_name = function(x) {\n  x = as.character(x)\n\n  # mark Cyrillic\n  has_cyr = stri_detect_charclass(x, \"\\\\p{Script=Cyrillic}\")\n\n  # transliterate only Cyrillic -&gt; Latin\n  x[has_cyr] = stri_trans_general(x[has_cyr], \"Cyrillic-Latin\")\n\n  # strip accents for all, lowercase, trim, squish\n  x = stri_trans_general(x, \"Latin-ASCII\")\n  x = tolower(x)\n  x = str_squish(x)\n  trimws(x)\n}\n\nadd_query_similarity = function(data){\n  data |&gt;\n    mutate(\n      dn = normalize_name(display_name),\n      qn = normalize_name(query_name),\n      query_similarity   = 1 - stringdist(dn, qn, method = \"jw\")\n    ) |&gt;  # Jaro–Winkler\n    select(-dn, -qn)\n}\n\n# dubbel check op achternaam\n# als daar voor nog wat voor staat komt dat overeen met de voornaam\n# als de voornaam, komt dit overeen met initalen\n\n\noa_fetch_scholar = function(\n    scholar_name, \n    university_ids,\n    pause = 0\n  ){\n  # fetch oa scholar information for all name id pairs\n  hold = list()\n  for (id in university_ids){\n    if (pause &gt; 0) Sys.sleep(pause)\n    \n    hold[[id]] = tryCatch(\n      oa_fetch(\n        entity = 'author',\n        search = scholar_name,\n        affiliations.institution.id = id,\n        mailto = \"jos.slabbekoorn@ru.nl\"\n      ),\n      error = function(e) NULL,\n      warning = function(w) NULL\n    )\n  }\n  res = bind_rows(hold)\n  # combine the results and drop duplicates\n  if (ncol(res) &gt; 2){\n    res = res |&gt; \n      distinct(.keep_all = TRUE) |&gt;\n      mutate(query_name = scholar_name) |&gt;\n      add_query_similarity() |&gt;\n      arrange(-query_similarity) |&gt;\n      head(15)\n  }\n\n  return(res)\n}\n\n\nscholar_name = 'Amy Verdun'\nid = \"I121797337\"\nres = oa_fetch_scholar(scholar_name, id)\n\n\nlibrary(cli)\nscholars = scholar$naam |&gt; unique()\n\nscrape_scholars = function(scholars){\n  # configure loop\n  k = length(scholars)\n  hold = list()\n\n  # loop over all scholars\n  cli_alert(\"Starting now, at {Sys.time()}\")\n  cli_progress_bar(\"Scraping Scholars\", total = k, clear = FALSE)\n  for (scholar_name in scholars) {\n    tab = scholar |&gt; \n      filter(naam == scholar_name) |&gt;\n      select(naam:university_id)\n\n    university_ids = as.list(na.omit(tab$university_id))\n    if (length(university_ids) &gt; 0){\n      hold[[scholar_name]] = oa_fetch_scholar(scholar_name, university_ids)\n    }\n    cli_progress_update()\n  }\n\n  return(hold)\n}\n\n\nhold = scrape_scholars(scholars)\n\n\nclean institutions data\n\nhold2 = list()\n\nfor (name_ in names(hold)){\n  if (is.na(name_) || !nzchar(trimws(name_))) next\n  tab = hold[[name_]] \n\n  # if there is a table, than update the table\n  if (nrow(tab) &gt;= 1){\n    tab = tab |&gt;\n    filter(query_name == name_) |&gt;\n    distinct(.keep_all = TRUE) |&gt;\n    mutate(\n      last_known_institutions = map(\n        last_known_institutions,\n        ~ .x |&gt;\n          distinct(.keep_all = TRUE) |&gt;\n          mutate(\n            institution_id   = str_remove(id, \"^https://openalex.org/\"),\n            institution_name = display_name\n          ) |&gt;\n          select(id, institution_id, institution_name, country_code)\n      ),\n      institution_ids = map(last_known_institutions, ~ .x |&gt; pull(institution_id) |&gt; unique())\n    )\n  }\n\n  hold2[[name_]] = tab\n}\n\n# hold2"
  },
  {
    "objectID": "src/docs/content/process_names.html",
    "href": "src/docs/content/process_names.html",
    "title": "Data Processing Names",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n\nsource(\"src/utils/custom_functions.r\")\n\n# load and activate packages\nfpackage.check(c(\n  'tidyverse', 'readxl',  'stringr', \n  'lubridate'\n))"
  },
  {
    "objectID": "src/docs/content/process_names.html#getting-started",
    "href": "src/docs/content/process_names.html#getting-started",
    "title": "Data Processing Names",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n\nsource(\"src/utils/custom_functions.r\")\n\n# load and activate packages\nfpackage.check(c(\n  'tidyverse', 'readxl',  'stringr', \n  'lubridate'\n))"
  },
  {
    "objectID": "src/docs/content/process_names.html#functions",
    "href": "src/docs/content/process_names.html#functions",
    "title": "Data Processing Names",
    "section": "Functions",
    "text": "Functions\n\nParse Names\n\nparse_names = function(names){\n  particles &lt;- c(\n    \"de\",\"den\",\"der\",\"het\",\"te\",\"ten\",\"ter\",\n    \"van\",\"van de\",\"van den\",\"van der\",\"van 't\",\"van ’t\",\n    \"'t\",\"’t\",\n    \"von\",\"von der\",\"von den\",\n    \"la\",\"le\",\"du\",\"del\",\"della\",\"di\",\"da\",\"dos\",\"das\",\"de la\",\"de los\",\"de las\",\n    \"zu\",\"zum\",\"zur\"\n  )\n\n  # Normalization helper for matching\n  .normalize &lt;- function(x) {\n    x |&gt;\n      str_squish() |&gt;\n      str_replace_all(\"’\", \"'\") |&gt;\n      str_to_lower()\n    }\n\n  p_norm &lt;- .normalize(particles)\n\n  names |&gt;\n    mutate(\n      tokens_raw = str_split(naam, \"\\\\s+\"),\n      tokens_norm = map(tokens_raw, ~ .normalize(.x)),\n      n = map_int(tokens_raw, length),\n\n      # find the longest particle match immediately before the final token\n        particle_idx = map2_int(tokens_norm, n, function(tok, n_tok) {\n          if (n_tok &lt; 2) return(NA_integer_)\n          # check 3, 2, 1-token particles that end at position n_tok-1\n          for (k in 3:1) {\n            start &lt;- n_tok - k\n            end   &lt;- n_tok - 1\n            if (start &gt;= 1) {\n              cand &lt;- paste(tok[start:end], collapse = \" \")\n              if (cand %in% p_norm) return(start)\n            }\n          }\n          NA_integer_\n        }),\n\n        has_particle = !is.na(particle_idx),\n\n        first_name = pmap_chr(\n          list(tokens_raw, particle_idx, n),\n          function(tok, p_i, n_tok) {\n            end_giv &lt;- if (is.na(p_i)) n_tok - 1 else p_i - 1\n            if (end_giv &lt;= 0) tok[1] else paste(tok[seq_len(end_giv)], collapse = \" \")\n          }\n        ),\n\n        particle = pmap_chr(\n          list(tokens_raw, tokens_norm, particle_idx, n),\n          function(tok_raw, tok_norm, p_i, n_tok) {\n            if (is.na(p_i)) return(NA_character_)\n            # output particle in lowercase, canonicalised apostrophes\n            out &lt;- paste(tok_norm[p_i:(n_tok-1)], collapse = \" \")\n            out\n          }\n        ),\n\n        last_name = pmap_chr(\n          list(tokens_raw, n),\n          function(tok, n_tok) tok[n_tok]\n        )\n    ) |&gt;\n    select(naam, first_name, particle, last_name)\n}\n\n\n\nExtract Initials\n\nextract_initials = function(names){\n  names |&gt;\n    mutate(\n      initials = str_extract(\n          naam, \n          \"^((?:\\\\p{Lu}{1,2}\\\\.)+(?:-\\\\p{Lu}{1,2}\\\\.)*)(?=\\\\s)\"\n        ),\n      first_name = ifelse(is.na(initials), first_name, NA_character_)) |&gt;\n    relocate(initials, .after=naam)\n}\n\n\n\nPatch Names\n\npatch_names = function(names){\n  # read in dataset with corrections for name information\n  corrections = readRDS(file.path('data', 'utils', 'name_corrections.Rds'))\n\n  names |&gt; rows_update(corrections, by='naam', unmatched='ignore')\n}\n\n\nreadRDS(file.path('data', 'utils', 'name_corrections.Rds')) |&gt; head()\n\n\n  \n\n\n\n\n\nExtract Maiden Name\n\nextract_maiden_name = function(names){\n  # split last names\n  last_name_splits = str_split(names$last_name, '-', simplify = TRUE)\n\n  # add splits to names dataframe\n  names['last_name'] = last_name_splits[,1]\n  names['maiden_name'] = ifelse(\n    last_name_splits[,2] == '',\n    NA_character_,\n    last_name_splits[,2]\n  )\n\n  return(names)\n}"
  },
  {
    "objectID": "src/docs/content/process_names.html#application",
    "href": "src/docs/content/process_names.html#application",
    "title": "Data Processing Names",
    "section": "Application",
    "text": "Application\n\n# load data \ndir = file.path(\"data\", \"processed\")\nfile = list.files(dir, pattern = \"scholarid.Rds\", full.names = TRUE)\n\ndata = freadRDS(file) \n\nnames = data |&gt;\n  select(naam) |&gt;\n  arrange(naam) |&gt;\n  distinct() |&gt;\n  drop_na() |&gt;\n  parse_names() |&gt;\n  extract_initials() |&gt;\n  patch_names() |&gt;\n  extract_maiden_name()\n\nfsaveRDS(\n  names, \n  'names'\n)\n\n[1] \"SAVING: ./data/processed/20251014names.Rds\""
  },
  {
    "objectID": "src/docs/content/create_scholars.html",
    "href": "src/docs/content/create_scholars.html",
    "title": "Data Processing Scholars",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  605258 32.4    1371408 73.3         NA   715800 38.3\nVcells 1121862  8.6    8388608 64.0      16384  2012047 15.4\n\n\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n\n# load and activate packages\nfpackage.check(c(\n  'tidyverse', 'readxl', 'renv', 'stringr', \n  'janitor', 'lubridate'\n))"
  },
  {
    "objectID": "src/docs/content/create_scholars.html#getting-started",
    "href": "src/docs/content/create_scholars.html#getting-started",
    "title": "Data Processing Scholars",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  605258 32.4    1371408 73.3         NA   715800 38.3\nVcells 1121862  8.6    8388608 64.0      16384  2012047 15.4\n\n\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n\n# load and activate packages\nfpackage.check(c(\n  'tidyverse', 'readxl', 'renv', 'stringr', \n  'janitor', 'lubridate'\n))"
  },
  {
    "objectID": "src/docs/content/create_scholars.html#functions",
    "href": "src/docs/content/create_scholars.html#functions",
    "title": "Data Processing Scholars",
    "section": "Functions",
    "text": "Functions\n\nLoading in Raw Data\nThis function loads in Excel files containing information on scholars working in Dutch sociology and political science departments. The data have been collected at three points in time — 19 December 2022, 19 April 2024, and 1 October 2025 — to capture changes in staff composition over time. Each Excel file includes two sheets, one for Sociologie and one for Politicologie, which are read in separately, harmonized, and then concatenated into a single dataset that combines all disciplines and time points.\n\n# define tailored functions\nread_spreadsheet = function(filepath, sheet){  \n  readxl::read_excel(filepath, sheet = sheet) \n}\n\nharmonize_columns = function(data, discipline, date){\n  data = data |&gt;\n    clean_names() |&gt;\n    mutate(\n      discipline = discipline,\n      date = date\n    ) |&gt;\n    relocate(c(discipline, date), .before = 1)\n}\n\nfread = function(\n    files, \n    source,\n    disciplines = c( \"Politicologie\", \"Sociologie\"),\n    language = 'nl'\n  ){\n  # read in both sheets from excel files\n  hold = c()\n  for (file in files){\n    filepath = file.path(source, file)\n    date = ymd(str_split(file, pattern = \"_\")[[1]][1])\n\n    # each file contains two sheets, for each discipline\n    # read in both sheets and combine these data\n    disciplines = c( \"Politicologie\", \"Sociologie\")\n    chunks = c()\n    for (discipline in disciplines){\n      chunk = read_spreadsheet(filepath, discipline) |&gt;\n        harmonize_columns(discipline, date)\n\n      chunks[[discipline]] = chunk\n    }\n\n    hold[[file]] =  bind_rows(chunks)\n  }\n\n  data = bind_rows(hold)\n\n  if (!language %in% c('nl', 'en')) stop(\"language should be nl or en\")\n  # implement rename function for english column namesac\n  \n\n  return(data)\n}\n\n\n\nFix Scholar Names\nThis step standardizes scholar names to ensure consistent matching across sources. It first normalizes name particles common in Dutch and related languages (e.g., van, de, der, ten, ’t, le, el, op den) by converting them to lowercase when they appear between spaces (e.g., “Jan van Dijk”). Next, it corrects a curated set of frequent typos and formatting inconsistencies - such as fixing misplaced or missing initials, diacritics, and misspellings (e.g., “AJGM van Montfort” to “A.J.G.M. van Montfort,” “Lea Kroner” to “Lea Kröner”). The result is a cleaned naam field with harmonized capitalization and corrected names, improving join accuracy and downstream deduplication.\n\nfix_scholar_names = function(data){\n  # fix the particles in the names\n  particles = c(\n    \"van\", \"in\",\n    \"de\", \"den\", \"der\", \"den\", \"del\",\n    \"te\", \"ten\", \"ter\", \"tes\", \"'t\",\n    \"la\", \"le\", \"les\", \"los\", \"el\", \"el-\",\n    \"op den\"\n  )\n  pattern = regex(\n    paste0(\"(?&lt;=\\\\s)(\", paste(particles, collapse = \"|\"), \")(?=\\\\s)\"),\n    ignore_case = TRUE\n  )\n\n  data |&gt;\n    mutate(\n      # replace capitalized particles\n      naam = str_replace_all(naam, pattern, ~ tolower(.x)),\n\n      # replace typos and mistakes in names\n      naam = case_when(\n        naam == \"Andrea Forstrer\"      ~ \"Andrea Forster\",\n        naam == \"AJGM van Montfort\"    ~ \"A.J.G.M. van Montfort\",\n        naam == \"FP Wagenaar\"          ~ \"F.P. Wagenaar\",\n        naam == \"JP Presley\"           ~ \"J.P. Presley\",\n        naam == \"JS Timmer\"            ~ \"J.S. Timmer\",\n        naam == \"ilya Lavrov\"          ~ \"Ilya Lavrov\",\n        naam == \"p. Vila Soler\"        ~ \"P. Vila Soler\",\n        naam == \"Z Dong\"               ~ \"Z. Dong\",\n        naam == \"Renae  Loh\"           ~ \"Renae Loh\",\n        naam == \"Paulina Pankowski\"    ~ \"Paulina Pankowska\",\n        naam == \"M.M Cuperus\"          ~ \"M.M. Cuperus\",\n        naam == \"Lea Kroner\"           ~ \"Lea Kröner\",\n        naam == \"L Slot\"               ~ \"L. Slot\",\n        naam == \"Jan Willen Duyvendak\" ~ \"Jan Willem Duyvendak\",\n        .default = naam\n      )\n    )\n}\n\n\n\nFixing Google Scholar ID\nThis step propagates known Google Scholar IDs across time for the same person. The data are first ordered by naam and date, then grouped by naam so that each individual’s records form a sequence. Within each group, missing values in google_scholar_id are filled using the nearest available value, with the fill direction controlled by the .direction argument (default “updown” fills forward and backward; alternatives like “down” or “up” restrict the fill to one direction). Groups are then ungrouped and the dataset is tidied by universiteit and date. This reduces missing IDs while ensuring values never leak across different people; it assumes that identical names refer to the same scholar, so remaining homonyms should be checked upstream.\n\nfix_google_scholar_id = function(data, .direction='updown'){\n  data |&gt;\n    arrange(naam, date) |&gt;\n    group_by(naam) |&gt;\n    # fill missing values with available information\n    fill(google_scholar_id, .direction = .direction) |&gt;  \n    ungroup() |&gt;\n    arrange(universiteit, date)\n}\n\n\n\nFixing Email Addresses\nThis step standardizes and completes the email address information for each scholar. It first applies a practical, case-insensitive regular expression that captures valid email formats, including subdomains, to extract clean addresses from the email_adres field. All extracted emails are then converted to lowercase to ensure consistency. Next, the data are grouped by universiteit and naam, and missing email values are filled using the nearest available information within each group (by default in both directions, controlled by the .direction argument). Finally, the dataset is ungrouped and ordered by universiteit and date, resulting in a harmonized and more complete set of email addresses that align across time points for the same scholar\n\nfix_email_adresses = function(data, .direction = \"updown\"){\n  # practical email regex (case-insensitive), supports subdomains\n  email_pattern = regex(\n    \"\\\\b[[:alnum:]._%+-]+@[[:alnum:]-]+(?:\\\\.[[:alnum:]-]+)+\\\\b\", \n    ignore_case = TRUE\n  )\n\n  # clean email variable\n  data |&gt;\n    mutate(\n      email_adres = str_extract(\n        email_adres, email_pattern\n        ) |&gt; tolower()\n    ) |&gt;\n    group_by(universiteit, naam) |&gt;\n    # fill \n    fill(email_adres, .direction = \"updown\") |&gt;\n    ungroup()|&gt;\n    arrange(universiteit, date)\n}\n\n\n\nFixing Universities\nThis step harmonizes university affiliations and adds a canonical code alongside the raw label. It first splits multi-valued entries in universiteit (e.g., “UU / UvA”) into separate rows, then trims and normalizes each label. Using a case-insensitive pattern, it maps recognized names/abbreviations to a standard set (EUR, RU, RUG, UU, VU, UvA, UvT, Leiden). If a canonical code is still missing, it infers the affiliation from the email domain (e.g., …@essb.eur.nl -&gt; EUR, …@vu.nl -&gt; VU, …@uva.nl -&gt; UvA, etc.). After removing duplicates, affiliations are re-aggregated per person and date into a list column university, joined back to the original data, and positioned next to universiteit. The result is a consistent, machine-readable university code that supports reliable grouping, filtering, and longitudinal comparison.\n\nclean_universities = function(data){\n  universities = c(\"EUR\", \"RU\", \"RUG\", \"UU\", \"VU\", \"UvA\", \"UvT\", \"LU\")\n  pat   = regex(\"\\\\b(EUR|RU|RUG|UU|VU|UvA|UvT|Leiden)\\\\b\", ignore_case = TRUE)\n  canon = setNames(universities, universities)\n\n  # clean universities\n  uni = data |&gt;\n    # split university strings on '\\s' , '/', '\\.', and '?'\n    mutate(\n      universiteit = str_split(universiteit, \"/+|\\\\?+\"),\n    ) |&gt;\n    unnest_longer(universiteit) |&gt;\n    # clean the university labels\n    mutate(\n      universiteit = str_replace(str_squish(universiteit), 'Leiden uni', 'Leiden'),\n      university = str_replace(universiteit, pat, \\(m) canon[str_to_lower(m)]),\n      universiteit = case_when(\n        (is.na(university) & str_detect(email_adres, 'essb.eur.nl')) ~ 'EUR',\n        (is.na(university) & str_detect(email_adres, 'vu.nl'))  ~ 'VU',\n        (is.na(university) & str_detect(email_adres, 'uva.nl'))  ~  'UVA',\n        (is.na(university) & str_detect(email_adres, 'leidenuni'))  ~  'Leiden',\n        (is.na(university) & str_detect(email_adres, 'ru.nl'))  ~  'RU',\n        (is.na(university) & str_detect(email_adres, 'rug.nl'))  ~  'RUG',\n        (is.na(university) & str_detect(email_adres, 'tilburguni'))  ~  'UvT',\n        (is.na(university) & str_detect(email_adres, 'uu.nl'))  ~  'UU',\n        .default = university\n      ),\n      university = ifelse(\"\" == university, NA_character_, university)\n    ) |&gt;\n    distinct(.keep_all=TRUE) |&gt;\n    group_by(naam, date) |&gt; \n    summarise(university = list(unlist(university))) |&gt;\n    ungroup()\n\n  data |&gt;\n    left_join(uni) |&gt;\n    relocate(university, .after=universiteit)\n}\n\n\n\nClean functie\nThis step parses free-text job titles in functie into a set of consistent role flags. It first keeps only date, naam, and functie, then lowercases functie for keyword matching. Using targeted patterns in Dutch and English, it creates boolean indicators for common categories:\n\nvisiting (e.g., gast, visit),\nexternal (buiten, external),\nprofessor (incl. typos like proffessor),\nassociate professor (hoofddocent, associate, uhd),\nassistant professor (universitair docent, assistant),\npostdoc (postdoc, doctoral),\nlecturer (docent, lecturer, teacher),\nresearcher (onderzoeker, research),\nPhD (phd, promovend),\nsenior/junior,\nemeritus (professor + emiri) or endowed (professor + bijzon),\nfellow,\nand a broad staff (e.g., advisor, secretary, assistent, medewerker, manager, coordinator, director/directeur).\n\nTo avoid double counting, several flags are explicitly set to FALSE when a more specific academic rank applies (e.g., assistant/associate/full professor precedence over lecturer/researcher/staff). Missing titles propagate as NA in the corresponding flags. The result is a tidy, machine-readable set of role indicators that standardizes heterogeneous job titles for downstream classification and analysis.\n\nparse_job_titles = function(data){\n  data |&gt;\n    select(date, naam, functie) |&gt;\n    mutate(\n      is_visiting = case_when(\n        str_detect(str_to_lower(functie), 'gast') ~ TRUE,\n        str_detect(str_to_lower(functie), 'visit') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_external = case_when(\n        str_detect(str_to_lower(functie), 'external') ~ TRUE,\n        str_detect(str_to_lower(functie), 'buiten') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_associate_professor = case_when(\n        str_detect(str_to_lower(functie), 'hoofddocent') ~ TRUE,\n        str_detect(str_to_lower(functie), 'associate ') ~ TRUE,\n        str_detect(str_to_lower(functie), 'uhd') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_assistant_professor = case_when(\n        is_associate_professor ~ FALSE,\n        str_detect(str_to_lower(functie), 'universitair docent') ~ TRUE,\n        str_detect(str_to_lower(functie), 'assistant ') ~ TRUE, \n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_postdoc = case_when(\n        str_detect(str_to_lower(functie), 'postdoc') ~ TRUE,\n        str_detect(str_to_lower(functie), 'doctoral') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_senior = case_when(\n        str_detect(str_to_lower(functie), 'senior') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_junior = case_when(\n        str_detect(str_to_lower(functie), 'junior') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_lecturer = case_when(\n        is_associate_professor ~ FALSE, \n        is_assistant_professor ~ FALSE, \n        str_detect(str_to_lower(functie), 'lecturer') ~ TRUE,\n        str_detect(str_to_lower(functie), 'docent') ~ TRUE,\n        str_detect(str_to_lower(functie), 'teacher') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_researcher = case_when(\n        is_associate_professor ~ FALSE, \n        is_assistant_professor ~ FALSE,\n        is_postdoc ~ FALSE, \n        str_detect(str_to_lower(functie), 'onderzoeker') ~ TRUE,\n        str_detect(str_to_lower(functie), 'research') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_phd = case_when(\n        str_detect(str_to_lower(functie), 'phd') ~ TRUE,\n        str_detect(str_to_lower(functie), 'promovend') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_professor = case_when(\n        is_associate_professor ~ FALSE, \n        is_assistant_professor ~ FALSE,\n        is_postdoc ~ FALSE, \n        str_detect(str_to_lower(functie), 'hoogleraar') ~ TRUE,\n        str_detect(str_to_lower(functie), 'professor') ~ TRUE,\n        str_detect(str_to_lower(functie), 'proffessor') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_emeritus = case_when(\n        is_professor & str_detect(str_to_lower(functie), 'emiri') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_endowed = case_when(\n        is_professor & str_detect(str_to_lower(functie), 'bijzon') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      ),\n      is_staff = case_when(\n        # make sure that people with other positions are not falsely\n        # been configured to be a staff member.\n        is_associate_professor ~ FALSE, \n        is_assistant_professor ~ FALSE,\n        is_lecturer ~ FALSE,\n        is_postdoc ~ FALSE,\n        is_professor ~ FALSE,\n        # staff members have wildly varying job titles.\n        str_detect(str_to_lower(functie), 'advisor') ~ TRUE,\n        str_detect(str_to_lower(functie), 'secretary') ~ TRUE,\n        str_detect(str_to_lower(functie), 'assistent') ~ TRUE,\n        str_detect(str_to_lower(functie), 'medewerk') ~ TRUE,\n        str_detect(str_to_lower(functie), 'market') ~ TRUE,\n        str_detect(str_to_lower(functie), 'managing') ~ TRUE,\n        str_detect(str_to_lower(functie), 'manager') ~ TRUE,\n        str_detect(str_to_lower(functie), 'coordinator') ~ TRUE,\n        str_detect(str_to_lower(functie), 'director') ~ TRUE,\n        str_detect(str_to_lower(functie), 'directeur') ~ TRUE,\n      ),\n      is_fellow = case_when(\n        str_detect(str_to_lower(functie), 'fellow') ~ TRUE,\n        is.na(functie) ~ NA,\n        .default = FALSE\n      )\n    )\n}\n\nBuilding on the role flags derived in Clean functie, this step converts those boolean indicators into human-readable position labels and, optionally, a compact string that also captures distinctions.\n\nconstruct_positions() maps the mutually exclusive academic roles to a canonical position (e.g., Full Professor, Associate Professor, Assistant Professor, Postdoctoral Researcher, PhD Candidate, Lecturer, Researcher, Staff) while separately flagging distinctions as short text labels: Visiting, External, Senior, Junior, Emeritus, Endowed, and Fellow.\n\nAfter creating these columns, it drops the original is_ flags and constructs position2 by uniting any present distinctions (from visiting through fellow) into a single, comma-separated string (leaving the per-column distinction flags intact for auditing).\n\nclean_functie() then orchestrates the full cleaning. It first runs parse_job_titles() to produce the role flags, passes the result through construct_positions(), and finally writes the output back into the original data:\n.what = \"complete\" (default) sets functie to the canonical position (rank only).\n.what = \"simplified\" sets functie to position2, which includes any distinctions (e.g., Assistant Professor, Visiting; Fellow).\n\nThe result is a consistent functie column suitable either for strict rank analyses (complete) or for descriptive reporting that preserves visiting/external/fellow/etc. qualifiers (simplified).\n\nconstruct_positions = function(data) {\n  data |&gt;\n    mutate(\n      # make flags for people with one of the following distinctions\n      visiting = ifelse(is_visiting, 'Visiting', NA_character_),\n      external = ifelse(is_external, 'External', NA_character_),\n      senior = ifelse(is_senior, 'Senior', NA_character_),\n      junior = ifelse(is_junior, 'Junior', NA_character_),\n      emeritus = ifelse(is_emeritus, 'Emeritus', NA_character_),\n      endowed = ifelse(is_endowed, 'Endowed', NA_character_),\n      # create a basic positions variable, excluding distinctions\n      position = case_when(\n        is_professor ~ \"Full Professor\",\n        is_associate_professor ~ \"Associate Professor\",\n        is_assistant_professor ~ \"Assistant Professor\",\n        is_postdoc ~ \"Postdoctoral Researcher\",\n        is_phd ~ \"PhD Candidate\",\n        is_lecturer ~ \"Lecturer\",\n        is_researcher ~ \"Researcher\",\n        is_staff ~ \"Staff\",\n        .default = NA_character_\n      ),\n      fellow = ifelse(is_fellow, 'Fellow', NA_character_)\n    ) |&gt;\n    select(!starts_with('is_')) |&gt;\n    unite('position2', visiting:fellow, na.rm=TRUE, remove=FALSE)\n}\n\nclean_functie = function(data, .what='complete'){\n  test = data |&gt; \n    parse_job_titles() |&gt;\n    construct_positions()\n\n  if (.what == 'complete'){\n    data['functie'] = test$position\n  } else if (.what == 'simplified'){\n    data['functie'] = test$position2\n  }\n\n  return(data)\n}"
  },
  {
    "objectID": "src/docs/content/create_scholars.html#application",
    "href": "src/docs/content/create_scholars.html#application",
    "title": "Data Processing Scholars",
    "section": "Application",
    "text": "Application\n\n# identified scholarid files\nsource = file.path(\"data\", \"raw_data\")\nfiles = list.files(source, pattern = \"scholarid.xlsx\")\n\n# load and process data\ndata = fread(files, source) |&gt; \n  fix_scholar_names() |&gt;\n  clean_universities() |&gt;\n  fix_email_adresses() |&gt;\n  fix_google_scholar_id() |&gt;\n  select(-specialisatie, -notitie, -additional, -checked) |&gt;\n  arrange(discipline, date, naam, university) |&gt;\n  clean_functie()\n\n`summarise()` has grouped output by 'naam'. You can override using the\n`.groups` argument.\nJoining with `by = join_by(date, naam)`\n\n# save data\nfsaveRDS(data, 'scholarid')\n\n[1] \"SAVING: ./data/processed/20251014scholarid.Rds\""
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scientific Co-Publishing Networks",
    "section": "",
    "text": "Note\n\n\n\nProject Description goes here"
  },
  {
    "objectID": "src/docs/content/scrape_works.html",
    "href": "src/docs/content/scrape_works.html",
    "title": "Data Preparation OpenAlex-ID",
    "section": "",
    "text": "Tip\n\n\n\nrough first working code for the collection of scholar_ids using openalexr\nI optimized the algorithm to not just select the top case, but allows for multiple ids (in rows) per person, in the decision rules i use: - semantic similarity - matches of university_id and institution_id"
  },
  {
    "objectID": "src/docs/content/scrape_works.html#getting-started",
    "href": "src/docs/content/scrape_works.html#getting-started",
    "title": "Data Preparation OpenAlex-ID",
    "section": "Getting Started",
    "text": "Getting Started\n\n# clear the global environment\nrm(list = ls())\ngc()\n\n\nsource(\"src/utils/custom_functions.r\")\n\n# load and activate packages\nfpackage.check(c(\n  'tidyverse', 'readxl',  'stringr', \n  'lubridate', 'openalexR', 'rvest', 'jsonlite',\n  'cli'\n))\n\noptions(openalexR.mailto = \"jos.slabbekoorn@ru.nl\")\n\n\nold_file_ = \"/Users/josslabbekoorn/Downloads/scholars_20240925.rda\"\nold = load(old_file_)\n\n\nfile = file.path('data', 'raw_data', '20251015oascholars.Rds')\nscholars = freadRDS(file) |&gt; bind_rows()\n\n\nsplits = scholars[1:5]\n\noa_fetch_works = function(scholars){\n    works = list()\n    ids = scholars$id |&gt; unique()\n    k = length(ids)\n\n    cli_alert(\"Starting now, at {Sys.time()}\")\n    cli_progress_bar(\"Scraping Works\", total = k, clear = FALSE)\n    for (id_ in ids){\n        tab = scholars |&gt; filter(id == id_)\n        \n        if (nrow(tab) &gt; 0){\n            for (i in 1:nrow(tab)){\n                row = tab[i, ]\n\n                res = tryCatch(\n                    oa_fetch(\n                        entity = 'works',\n                        author.id = str_remove(id_, 'https://openalex.org/'),\n                        mailto = \"jos.slabbekoorn@ru.nl\"\n                    ),\n                    error = function(e) NULL,\n                    warning = function(w) NULL\n                    )\n                }\n\n                if (!is.null(res)){\n                    res = res |&gt;\n                        mutate(author_id = id_) |&gt;\n                        relocate(author_id, .before=id)\n                }\n                works[[id_]] = res\n            \n        }\n        cli_progress_update()\n    }\n    \n    return(works)\n}\n\n\nworks = oa_fetch_works(scholars)\n\n\nfsaveRDS(works, 'oaworks', location = \"./data/raw_data/\")"
  },
  {
    "objectID": "src/docs/content/combine_data.html",
    "href": "src/docs/content/combine_data.html",
    "title": "Data Preparation OpenAlex-ID",
    "section": "",
    "text": "Tip\n\n\n\nrough first working code for the collection of scholar_ids using openalexr\nI optimized the algorithm to not just select the top case, but allows for multiple ids (in rows) per person, in the decision rules i use: - semantic similarity - matches of university_id and institution_id"
  },
  {
    "objectID": "src/docs/content/combine_data.html#getting-started",
    "href": "src/docs/content/combine_data.html#getting-started",
    "title": "Data Preparation OpenAlex-ID",
    "section": "Getting Started",
    "text": "Getting Started\n\n# clear the global environment\nrm(list = ls())\ngc()\n\n\nsource(\"src/utils/custom_functions.r\")\n\n# load and activate packages\nfpackage.check(c(\n  'tidyverse', 'readxl',  'stringr', \n  'lubridate', 'openalexR', 'rvest', 'jsonlite',\n  'cli'\n))\n\noptions(openalexR.mailto = \"jos.slabbekoorn@ru.nl\")\n\n\ndir = file.path('data', 'processed')\nfiles = list.files(dir)\ndata = list(\n    scholars = readRDS(file.path(dir, files[str_detect(files, 'scholarid')])),\n    ethnicity = readRDS(file.path(dir, files[str_detect(files, 'ethnicity')])),\n    gender = readRDS(file.path(dir, files[str_detect(files, 'gender')])),\n    names = readRDS(file.path(dir, files[str_detect(files, 'names')])),\n    id = readRDS(file.path('data', 'raw_data', '20251015oascholars.Rds')),\n    works = readRDS(file.path('data', 'raw_data', '20251016oaworks.Rds'))\n)\n\n\n# scholars = fread(file.)\n\n\ndem = data[['scholars']] |&gt;\n    mutate(\n        year = as.integer(year(date) - 2000)\n    ) |&gt;\n    arrange(naam, date) |&gt;\n    select(-university, -date, -google_scholar_id) |&gt;\n    pivot_wider(\n        names_from  = year,\n        values_from = c(email_adres, universiteit, functie, discipline),\n        names_glue  = \"{.value}.{year}\",\n        values_fill = NA,\n        values_fn = list(\n        email_adres   = ~ if (all(is.na(.x))) NA_character_ else str_c(unique(na.omit(.x)), collapse = \"; \"),\n        universiteit  = ~ if (all(is.na(.x))) NA_character_ else first(na.omit(.x)),\n        functie       = ~ if (all(is.na(.x))) NA_character_ else first(na.omit(.x)),\n        discipline    = ~ if (all(is.na(.x))) NA_character_ else first(na.omit(.x))\n        )\n    ) |&gt;\n    select(\n        naam, ends_with('22'), ends_with('24'), ends_with('25')\n    )\n\n\n# add ids\ndems = data$id |&gt;\n    bind_rows() |&gt;\n    arrange(query_name, works_count) |&gt;\n    select(query_name, id) |&gt;\n    # select the row with the most works\n    distinct(query_name, .keep_all = TRUE) |&gt;\n    # merge in demographics\n    rename(naam = query_name) |&gt;\n    right_join(dem) |&gt;\n    # merge in names\n    left_join(data$names) |&gt;\n    relocate(initials:maiden_name, .after=id) |&gt;\n    # merge in gender\n    left_join(data$gender |&gt; select(-count, -prob)) |&gt;\n    relocate(gender, .after=maiden_name) |&gt;\n    # merge in ethnicity\n    left_join(data$ethnicity |&gt; select(-name_count)) |&gt;\n    relocate(origin:dutch, .after=gender) |&gt;\n    distinct(naam, id, .keep_all=TRUE) |&gt;\n    mutate(has_oa_id = !is.na(id))\n\n\nids = na.omit(unique(dems$id))\nworks = list()\nfor (id in ids){\n    works[[id]] = data$works[[id]]\n}\n\n\nauthor_list = list()\ndf_scholars = bind_rows(data$id)\nfor (id_ in ids){\n    tab = df_scholars |&gt; filter(id == id_)\n    author_list[[id_]] = tab\n}\n\n\nscholars = list(\n    demographics=dems, \n    scholars_oa = author_list,\n    works = works\n)"
  },
  {
    "objectID": "src/docs/content/process_ethnicity.html",
    "href": "src/docs/content/process_ethnicity.html",
    "title": "Data Processing Ethnicity",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  605262 32.4    1371420 73.3         NA   715800 38.3\nVcells 1119043  8.6    8388608 64.0      16384  2012047 15.4\n\n\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n\n# load and activate packages\nfpackage.check(c(\n  'tidyverse', 'readxl',  'stringr', \n  'lubridate', 'httr2', 'rvest', 'xml2',\n  \"purrr\", \"RCurl\", \"fuzzyjoin\", \"stringi\"\n))"
  },
  {
    "objectID": "src/docs/content/process_ethnicity.html#getting-started",
    "href": "src/docs/content/process_ethnicity.html#getting-started",
    "title": "Data Processing Ethnicity",
    "section": "",
    "text": "# clear the global environment\nrm(list = ls())\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  605262 32.4    1371420 73.3         NA   715800 38.3\nVcells 1119043  8.6    8388608 64.0      16384  2012047 15.4\n\n\n\n# load custom functions\nsource(\"src/utils/custom_functions.r\")\n\n# load and activate packages\nfpackage.check(c(\n  'tidyverse', 'readxl',  'stringr', \n  'lubridate', 'httr2', 'rvest', 'xml2',\n  \"purrr\", \"RCurl\", \"fuzzyjoin\", \"stringi\"\n))"
  },
  {
    "objectID": "src/docs/content/process_ethnicity.html#functions",
    "href": "src/docs/content/process_ethnicity.html#functions",
    "title": "Data Processing Ethnicity",
    "section": "Functions",
    "text": "Functions\n\nScraper Configuration\nThis function builds a resilient GET request to a target page and returns a uniform result object instead of throwing on transport errors.\n\nSuccess check. is_ok() flags responses with HTTP status in the 200–299 range.\nBrowser-like request. request_last_name() constructs an httr2 request with a Mac Chrome user agent, Dutch/English Accept-Language, and broad Accept headers to mimic a real browser and reduce blocking.\nConnection settings. It sets a 30-second timeout and (deliberately) disables SSL peer verification to cope with misconfigured certificates on legacy servers.\nRetry policy. On transient errors (HTTP 429 or 5xx), it retries up to 4 times with jittered exponential backoff (runif(0.5–1.2) × 2^(try−1)), which spreads load and avoids thundering herds.\nPoliteness delay. If a global pause is set, it sleeps pause + U(0, 0.4) seconds before firing the request to throttle scraping.\nError handling. The actual HTTP call is wrapped in tryCatch(). Instead of stopping, it returns a structured list:\n\nok (logical) – success per is_ok()\nstatus (integer) – HTTP status or NA on transport error\nurl (character) – the requested URL\nresp (httr2 response) – raw response on success, NULL on error\nerror (condition) – the caught error on failure\n\n\nThis design lets downstream code branch cleanly on res$ok and inspect res$status or res$error without breaking the pipeline.\n\nis_ok = function(resp) resp_status(resp) &gt;= 200 && resp_status(resp) &lt; 300\n\nrequest_last_name = function(base_url, pause=0.5){\n  # configure user agent\n  ua = paste(\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 15_5)\",\n    \"AppleWebKit/537.36 (KHTML, like Gecko)\",\n    \"Chrome/129.0.0.0 Safari/537.36\"\n  )\n\n  req = request(base_url) |&gt;\n    req_user_agent(ua) |&gt;\n    # disable SSL verification\n    req_options(ssl_verifypeer = 0) |&gt;\n    req_timeout(30) |&gt;\n    req_headers(\n      \"Accept\" = \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n      \"Accept-Language\" = \"nl,en;q=0.8\"\n    ) |&gt;\n    req_retry(\n      max_tries = 4,\n      backoff = ~ runif(1, 0.5, 1.2) * (2 ^ (.x - 1)),  # jittered exponential\n      is_transient = function(resp) {\n        code &lt;- resp_status(resp)\n        isTRUE(code == 429L || (code &gt;= 500L & code &lt; 600L))\n      }\n    )\n\n  # polite pause + jitter\n  if (pause &gt; 0) Sys.sleep(pause + runif(1, 0, 0.4))\n\n  # CRITICAL: don't throw on transport errors\n  resp &lt;- tryCatch(\n    req_perform(req),\n    error = function(e) {\n      attr(e, \"nvb_url\") &lt;- url\n      e\n    }\n  )\n\n  # Return a uniform list the caller can inspect\n  if (inherits(resp, \"error\")) {\n    res = list(ok = FALSE, status = NA_integer_, url = url, resp = NULL, error = resp)\n  } else {\n    res = list(ok = is_ok(resp), status = resp_status(resp), url = url, resp = resp, error = NULL)\n  }\n  return(res)\n}\n\n\n\nConfigure URL and Extracters\n\nformat_url() – Build a query URL for CBG Familienamen\nTakes a name and constructs a browser-ready URL for the CBG surnames site, encoding spaces as + and adding query parameters for multiple name fields. Optional .what = \"info\" appends the path to the analysis/etymology page. Returns the full URL string.\n\nformat_url = function(\n    name,\n    base = \"https://www.cbgfamilienamen.nl/nfb/detail_naam.php?\",\n    .what = \"base\"\n  ){\n  # format_name for URL\n  formatted_name = name |&gt; \n    URLencode(reserved = TRUE) |&gt;\n    str_replace_all(pattern = \"%20\", '+')\n\n  # configure url\n  url = paste0(\n    base,\n    \"gba_naam=\", formatted_name,\n    \"&gba_lcnaam=\", tolower(formatted_name),\n    \"&nfd_naam=\", formatted_name\n  )\n\n  if (.what == 'info'){\n    url = paste0(url, \"&info=analyse+en+verklaring\")\n  }\n  \n  return(url)\n}\n\n\n\nextract_info() – Parse analysis/etymology text from response\nGiven a response wrapper r (with r$resp from httr2), reads the HTML body and extracts the “kenmerken/verklaring” section. It splits the page text, trims boilerplate until the © footer, and returns a single “;”-separated string with the extracted info (or “” if not found).\n\nextract_info = function(r){\n  html = read_html(resp_body_string(r$resp))\n  text = html |&gt;\n    html_element(\"body\") |&gt;\n    html_text()\n\n  info = c(\"\")\n  if (str_detect(text, regex('kenmerken:|verklaring:'))){\n    parts = str_split_fixed(text, pattern=regex('kenmerken:|verklaring:'), 2)[2] |&gt;\n      str_split(pattern = regex(\"[\\\\n|\\\\t|\\\\s]{3,20}\")) |&gt;\n      unlist()\n\n    idx = which(grepl(regex(\"©\"), parts))\n\n    if (length(idx) &gt;= 1) {\n      info = parts[seq_len(idx[1] - 1)]\n      info = as.vector(info[nzchar(info)])\n    }\n  }\n\n  # drop empty strings\n  # info = info[nzchar(info)]\n  return(paste(unname(info), collapse = \"; \"))\n}\n\n\n\nextract_count() – Retrieve occurrence count from tables\nParses all HTML tables from r$resp and searches for the table layout containing the national count. If found, pulls the value at row 2, column 2 (per the site’s structure) and returns it as an integer; throws an error if no tables are present and returns NA when the expected layout is missing.\n\nextract_count = function(r){\n  # extract all tables on the page\n  html = read_html(resp_body_string(r$resp))\n  tables = html_table(html, header=FALSE)\n\n  # select the first table if a table if found\n  if (length(tables) == 0) stop(\"No tables were found\")\n  \n  # set count value\n  count = NA_integer_\n  if (length(tables) &gt;= 4) {\n    for (tab in tables){\n      i = nrow(tab)\n      if (i &gt;= 5){\n        count = tab[2,2] |&gt; pull()\n      }\n    }\n  }\n\n  return(count)\n}\n\n\n\n\nTidy Scrape Wrappers\nThese utilities wrap the earlier extractors and assemble a tidy result for a single surname. - safe_count() / safe_info() wrap extract_count() and extract_info() in tryCatch(), returning a default (NA_integer_ / NA_character_) on any error or warning. This guarantees downstream code receives a value even when pages are missing or malformed. - get_name_row() takes a name and performs up to two requests: 1. Builds the base URL with format_url(name) and fetches it via request_last_name(). If the HTTP result is ok, it parses the national occurrence count with safe_count(); otherwise it records NA. 2. Only if a non-missing count was obtained, it builds the “info” URL (.what = \"info\") and requests it, then extracts the analysis/etymology info with safe_info() (again only when both requests succeeded).\nBoth count and info are coerced to character for consistency. The function returns a one-row tibble with last_name, name_count, and info, providing a compact, fault-tolerant record for each queried surname.\n\nsafe_count = function(r, default = NA_integer_) {\n  tryCatch(extract_count(r),\n           error = function(e) default,\n           warning = function(w) default)\n}\n\nsafe_info = function(r, default = NA_character_) {\n  tryCatch(extract_info(r),\n           error = function(e) default,\n           warning = function(w) default)\n}\n\nget_name_row = function(name, count=NA_character_, info=NA_character_){\n  # scrape the count information\n  r1 = format_url(name) |&gt;\n    request_last_name() \n\n  count = if (isTRUE(r1$ok)) safe_count(r1) else NA_character_\n  count = as.character(count)\n\n  # scrape info if the first scrape yielded success\n  if (!is.na(count)){\n  r2 = format_url(name, .what=\"info\") |&gt;\n    request_last_name()\n\n  info = if (isTRUE(r2$ok) && isTRUE(r1$ok)) safe_info(r2) else NA_character_\n  info = as.character(info)\n  }\n\n\n  as_tibble(list(\n    last_name = name, \n    name_count = count,\n    info = info\n  ))\n}\n\n\nadd_origin5 = function(ethnicity){\n  ethnicity_patch = readxl::read_excel(\n      file.path('data', 'utils', 'origin_patch.xlsx')\n    ) |&gt;\n    mutate(last_name_norm = normalize_name(last_name))\n\n  ethnicity |&gt;\n    mutate(last_name_norm = normalize_name(last_name)) |&gt;\n    fuzzyjoin::stringdist_left_join(\n      ethnicity_patch,\n      by = \"last_name_norm\",\n      max_dist = 0.5\n    ) |&gt;\n    rename(\n      \"last_name\" = \"last_name.x\",\n      \"origin5\" = \"origin\"\n    ) |&gt;\n    select(\n      -last_name_norm.x, -count, \n      -last_name_norm.y, -last_name.y,\n    )\n}\n\nadd_origin = function(data) {\n  ethnicity_cache |&gt;\n  mutate(\n    origin1 = str_extract_all(info, regex(\"[:upper:]([:lower:]{2,}) naam\")),\n    origin2 = ifelse(\n      str_detect(info, \"afkomstig uit\"),\n      str_remove(info, \".*afkomstig uit\"),\n      NA_character_\n    ),\n    origin3 = str_extract(\n      info, \"[:upper:]([:lower:]{2,}) (achter)?(familie)?(beroeps)?naam\"\n    )\n  ) |&gt;\n  # clean origin information\n  mutate(\n    # Exclude Jewish people form Origin information\n    origin1 = str_remove(origin1, \"Joodse naam\"),\n    # Only s\n    origin2 = str_remove(origin2, \"\\\\..*\") |&gt;\n      str_remove(\"\\\\;.*\") |&gt;\n      str_remove(\"\\\\(.*\"),\n    regional = str_detect(\n      origin2, paste0(\n        \"(dorp)|(plaats)|(gemeente)\",\n        \"|(graafschap)|(stad)|(deel)|(Friesland)\"\n      )\n    ),\n    has_particle = str_detect(\n      last_name, regex(\"^(van |de )\")\n    ),\n    origin2 = ifelse(isTRUE(regional), NA_character_, origin2),\n    origin3 = origin3 |&gt;\n      str_remove(\"D(i)?e(ze)? (familie)?(achter)?(beroeps)?naam\") |&gt;\n      str_remove(\"Een (familie)?(achter)?(beroeps)?naam\") |&gt;\n      str_remove(\"Zijn (familie)?(achter)?(beroeps)?naam\") |&gt;\n      str_remove(\"Als (familie)?(achter)?(beroeps)?naam\") |&gt;\n      str_remove(\"Joodse (familie)?(achter)?naam\") |&gt;\n      str_remove(\"Bijbelse (familie)?(achter)?naam\"),\n    origin4 = str_detect(info, \"andere taal\")\n  ) |&gt;\n  select(-regional)\n}\n\n\nclean_ethnicity = function(data){\n  data |&gt;\n    mutate(\n      origin1 = ifelse(origin1 == 'character(0)', NA_character_, origin1),\n      origin = case_when(\n        length(origin1) &gt; 1 ~ origin3,\n        .default = origin1\n      ),\n      origin = coalesce(origin, origin5),\n      origin = case_when(\n        is.na(origin) & !is.na(count) & !is.na(info) ~ \"Nederlandse naam\",\n        has_particle ~ \"Nederlandse naam\",\n        .default = origin\n      ),\n      dutch = case_when(\n        origin == 'Nederlandse naam' ~ TRUE,\n        .default = FALSE\n      ),\n      origin = str_remove(origin, regex(\"\\\\ .*\"))\n    ) |&gt;\n    select(last_name, origin, dutch, name_count)\n}"
  },
  {
    "objectID": "src/docs/content/process_ethnicity.html#application",
    "href": "src/docs/content/process_ethnicity.html#application",
    "title": "Data Processing Ethnicity",
    "section": "Application",
    "text": "Application\n\ndir = file.path('data', 'processed')\nfile = list.files(dir, pattern = 'names.Rds')[[1]]\nnames = readRDS(file.path(dir, file))\n\nlast_names = names |&gt;\n  distinct(particle, last_name) |&gt;\n  unite(last_name, particle:last_name, sep=\" \", na.rm=TRUE) |&gt;\n  filter(!is.na(last_name), last_name != \"\") |&gt;\n  pull(last_name) |&gt;\n  sort()\n\nethnicity_cache = readRDS(file.path('data', 'utils', 'cbg_cache.Rds'))\n\nhold = c()\nfor (name in last_names){\n  if (!name %in% ethnicity_cache$last_name){\n    hold[[name]] = get_name_row(name)\n  }\n}\n\nethnicity_cache = bind_rows(ethnicity_cache, bind_rows(hold))\nsaveRDS(ethnicity_cache,  file.path('data', 'utils', 'cbg_cache.Rds'))\n\n\nethnicity = ethnicity_cache |&gt;\n  add_origin() |&gt;\n  add_origin5() |&gt;\n  clean_ethnicity()\n\n\nfsaveRDS(ethnicity, 'ethnicity')\n\n[1] \"SAVING: ./data/processed/20251014ethnicity.Rds\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nA considerable amount of people do not have an ethnicity yet. an option could be to use genderizeR API to infer ethnicity."
  }
]